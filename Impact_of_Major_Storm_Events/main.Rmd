---
title: "Human and Economic Cost of Major Storm Events: An Analysis of the NOAA/NCDC Database"
subtitle : 
author   : Giovanni Fossati
job      : null
output   : 
  html_document:
    self_contained: false
    css: css/gf_small_touches.css
    theme: cerulean
    highlight: tango
    keep_md: no
    mathjax: default
    toc: no
---

```{r setup, cache = FALSE, echo = FALSE, message = FALSE, warning = FALSE, tidy = FALSE}
require(knitr)
options(width = 100, 
        digits = 7)
opts_chunk$set(message = FALSE, 
               error = FALSE, 
               warning = FALSE, 
               collapse = TRUE, 
               tidy = FALSE,
               cache = FALSE, 
               cache.path = '.cache/', 
               comment = '#',
               fig.align = 'center', 
               dpi = 100, 
               fig.path = 'figures/',
               dev="png", 
               dev.args=list(type="cairo"),
               dpi=96)
```

```{r loadLibs, echo = FALSE, cache = FALSE}
library("plyr")
library("reshape2")
library("ggplot2")
library("grid")
```

## PREAMBLE

Report for the first assignment of the [_Reproducible Research_](https://www.coursera.org/course/repdata) 
course of the _Coursera/JHSPH Data Science Specialization_.

The source files are posted on [GitHub](https://github.com/pedrosan/DataScienceExamples/tree/master/Impact_of_Major_Storm_Events).    


## Synopsis

This project involves exploring the U.S. National Oceanic and Atmospheric
Administration’s (NOAA) storm database. 
This database records major weather-related events in the United States, tracking their times and locations 
as well as estimates of fatalities, injuries, property and agricultural damages that may be
associated with them.

We seek to identify which type of storm events have the greatest impact on the population both
from the point of view of their health and of the economic consequences.
To address these questions, our analysis aggregates the data by storm event type
to identify the storm events categories responsible for the largest impact as measured by
_fatalities_, _injuries_, _property damage_, _crop damage_.

### Findings

Looking separately at each harm/damage category separately based on our analysis we can say that 
the most severe impact comes from the following types of events:

* __agricultural damage__: _hurricanes_ and _drought_, both in terms of total economic cost and fraction
of events that cause significant damage.  
It is worth noting that in addition to events directly classified as _hurricane_, among the
most impactful event types there are others that are/can be related to _hurricanes_ themselves or 
other kind of tropical storm activity (e.g. _tropical storms_, _storm surge_).

* __property damage__: in absolute terms _hurricanes_ and _floods_ dominate the economic impact (and here again
one may want to look at a broader "tropical event" category).
When we consider the likelihood that a storm causes "recordable" damages, _wind_ and _snow_ emerge
at the top.  While their cumulative cost may not be as substantial as that of major events like hurricanes, 
the damage caused by _wind_ and _snow_ can have a serious impact on the life and economic activity 
of a wider range of families and communities across the country and it warrants attention and 
adequate preparation.

* __human fatalities/injuries__: _tornadoes_, _floods_ (including _flash floods_) and _excessive heat_
emerge as the most serious threats to human health in absolute terms.
It is interesting however, to note that in terms of fraction of events with fatalities/injuries,
_rip currents_ and _avalanches_ take the top spots (with "likelihood" >= 50% of fatalities and about 25% of injuries 
on a given event).   
This may suggest that people expose themselves to avoidable risks, given that one can think 
of associating _rip currents_ and _avalanches_ with leisure activities.


### Future Work

The scope and time for this investigation were limited, but there are clear directions worth
further investigation, such as

* Geographic analysis, clearly necessary given the fairy localized impact of different type of events.
* Variation over time, perhaps reflecting changes in behavior, preparation and policies.


<hr class="thin_separator">
<a name="DATA_PROCESSING"></a>

## Data Processing

The data structure and definition documentation is at the following URLs of the 
NOAA National Climatic Data Center (NCDC): 

* [Storm Events Database](http://www.ncdc.noaa.gov/stormevents/details.jsp)
* [Storm Data Documentation](http://www.ncdc.noaa.gov/stormevents/pd01016005curr.pdf)
* [FAQ](http://www.ncdc.noaa.gov/oa/climate/sd/sdfaq.html)

The raw data comprise 37 variables with 902,297 observation from years 1950-2011.


### The data and the aim of our analysis

The objective of this research is to address two main questions:

* which events have the greatest health consequences measured by _fatalities_ and _injuries_.
* Which events have the greatest economic consequences measured by _property damage_,  _crop damage_, _total economic damage_.

The main variables in the NOAA/NCDC Storm Event databases that we will consider are: 

* For storm event date and type: 

    * __BGN_DATE__: begin date of event.
    * __EVTYPE__: event type (see discussion later).

* For human and economic damages associated with each storm event: 

    + __FATALITIES__: number of fatalities.
    + __INJURIES__: number of injuries.
    + __PROPDMG__: property damage assessment.
    + __CROPDMG__: agricultural damage assessment.
    
The values reported for these two latter have to be converted in actual US dollar amounts by 
multiplying them by a scaling factor coded in two other variables (`PROPDMGEXP`, `CROPDMGEXP`) as 
the power of 10 of the multiplier. These two auxiliary variables seem to suffer from some 
data-entry issues, which we will address below. 


### Loading the dataset

The data set comes as a compressed _csv_ file, which can be read directly with `read.csv()`.
After reviewing the data we set our preferred values for the classes of the variables loaded 
in the data-frame and we set them on loading via the `colClasses` option, using a character
vector read from an auxiliary file.

NOTE: given the substantial size of the data set we set the options `cache=TRUE` to reduce the 
`knitr` processing time to a manageable length.

```{r set-classes}
# col.cl <- read.csv("data/readin_classes.csv", stringsAsFactors=FALSE, strip.white=TRUE)
# col.cl$newclass2
```

```{r load-data, cache=TRUE}
col.cl <- read.csv("data/readin_classes.csv", stringsAsFactors=FALSE, strip.white=TRUE)
col.cl$newclass2

system.time( data <- read.csv("data/StormData.csv.bz2", colClasses= col.cl$newclass2, nrows=1300000, strip.white=TRUE) )
# data2 <- readRDS("data/df_good_saved.rds")
```

This is the structure of the data-frame as loaded.
```{r}
str(data)
```


### Cleaning, reformatting, and new variables

Due to the nature of the development of this dataset several variables suffer from inconsistent
formatting and in order to make our analysis as robust as possible we cleaned and reformatted
some of the potentially useful variables, as well as created ones for further convenience.

#### _REMARKS_ and _EVTYPE_: tidying up spaces and letters' case

```{r clean-1, cache=TRUE}
data$REMARKS <- gsub("^ *", "", data$REMARKS, perl=TRUE)
data$REMARKS <- gsub(" *$", "", data$REMARKS, perl=TRUE)
data$REMARKS <- gsub("[ ]{2,}", " ", data$REMARKS, perl=TRUE)

data$EVTYPE <- gsub("^ *", "", data$EVTYPE, perl=TRUE)
data$EVTYPE <- gsub(" *$", "", data$EVTYPE, perl=TRUE)
data$EVTYPE <- gsub("[ ]{2,}", " ", data$EVTYPE, perl=TRUE)

data$COUNTYNAME <- toupper(data$COUNTYNAME)
data$EVTYPE     <- toupper(data$EVTYPE)
data$PROPDMGEXP <- toupper(data$PROPDMGEXP)
data$CROPDMGEXP <- toupper(data$CROPDMGEXP)
```

#### Setting "missing" coordinates to _NA_

```{r clean-coords, cache = FALSE}
data$LATITUDE[data$LATITUDE <= 0]  <- NA
data$LONGITUDE[data$LONGITUDE <= 0]  <- NA
```

#### Times

First a small fix motivated by the discovery of `O` instead of a `0` in the `BGN_TIME` variable:

```{r clean-time, cache=TRUE}
data$BGN_TIME <- gsub("([0-9])O(.*)([^M])$","\\10\\2\\3", data$BGN_TIME, perl=TRUE)
```

It is better to work with properly constructed time variables, hence we convert `BGN_DATE` and 
`END_DATE` into `POSIXlt` class variables. 

```{r transform-dates, cache = FALSE}
# making BGN and END dates dates
data$BGN_DATE.new <- strptime(as.character(data$BGN_DATE), "%m/%d/%Y %H:%M:%S")
data$END_DATE.new <- strptime(as.character(data$END_DATE), "%m/%d/%Y %H:%M:%S")
```

Finally, we define a _factor_ variable `YEAR`.
```{r define-year-factor, cache = FALSE}
# create a 4-digit year factor variable
# data$YEAR <- as.factor(substr(as.character(data$BGN_DATE.new),0,4))
data$YEAR <- substr(as.character(data$BGN_DATE.new),0,4)
```

### Subsetting full data-frame to _good_ data-frame

For ease of processing it is practical to work with a reduced set of variables.  
Our choice was the following:
```{r selcolumns}
sel.columns.n <- c( 37, 7, 5, 40, 38, 39, 8, 23, 24, 25, 26, 27, 28, 36)
colnames(data)[sel.columns.n]
```

After creating the new data-frame, we rename the newly created time variables to take
the original names, and sort the data-frame by `BGN_DATE`.

```{r create-good-df, cache = FALSE}
good <- data[, sel.columns.n]

colnames(good)[5] <- "BGN_DATE"
colnames(good)[6] <- "END_DATE"

# sorting data frame chronologically 
good <- good[order(good$BGN_DATE), ]
```

```{r}
str(good)
```


#### The _EVTYPE_ problem!

We perform some more cleaning steps on this leaner data-frame to _regularize_ the `EVTYPE` variable.   
Because of historical reasons, namely the heterogenous sources of the data compiled into the Storm 
Event database, the `EVTYPE` variable requires a significant amount of work to tidy it up
into a more usable form.

While in theory since 1996 NOAA has codified a set of 48 type of events
[See NWS Directive](http://www.ncdc.noaa.gov/stormevents/pd01016005curr.pdf) to be used in the
classification of storm events, a quick review of the different `EVTYPE` values 
for each year shows that their number remained higher until much more recently.

```{r census-of-evtype, cache=TRUE}
EVTYPE.by.year <- tapply(good$EVTYPE, good$YEAR, function(x) table(x), simplify=TRUE)
sapply(EVTYPE.by.year, function(x) length(names(x)))
```

Until 1993 _"Tornado"_", _"Thunderstorm Wind"_" and _"Hail"_" type of events were recorded.
The 1993 and 1995 data have been extracted from unformatted text files, and this helps explain 
the high level of _noise_ of `EVTYPE` during those years. 
It is less clear why after the Directive of 1996 the data have not become 

A quick census of the most frequent `EVTYPE` can be extracted with `table()`

```{r evtype-census, cache=TRUE}
evt.census <- as.data.frame(table(data$EVTYPE))
head(evt.census[order(evt.census$Freq, decreasing=TRUE),],10)

nrow(evt.census)
```
The full database comprises `r nrow(evt.census)` unique `EVTYPE` values!   
It is immediately obvious the problem with inconsistent naming, as in the top-10 there
are three entries for the same storm event type, _Thunderstorm Wind_: `TSTM WIND`, 
`THUNDERSTORM WIND`, `THUNDERSTORM WINDS` .

After an extensive analysis of the `EVTYPE`, we put together a lengthy set of substitutions
via `gsub()` and `grepl()` to consolidate as many related types as possible.
We run this by calling an external script to avoid clogging the document, but we include
the source of the script in the Appendix.   
The script creates a new character vector added to the `good` data-frame as `EVTYPE`. 

* show percentage "recovered", number of `EVTYPE` per year, etc... 

```{r evtype-regularization}
# Calling 'grepl.R' script to clean the EVTYPE entries
source("./scripts/evtype_regularization.R")

# colnames(good)[7] <- "old.EVTYPE"
good$EVTYPE <- TESTvec
```

```{r}
str(good)

dim( table(good$EVTYPE) )
```

#### The _PROPDMGEXP_ and _CROPDMGEXP_ problem

The variables `PROPDMGEXP` and `CROPDMGEXP` are supposed to encode exponents for _power of 10 units_ 
for multiplying factors for their respective economic damage variables.    
They too suffer from inconsistent data entry in the _transition years_ around 1995 as it
can be seen by this quick analysis of the number of unique values for each year, accompanied 
by the list of values for the years with the highest number of them:
```{r census-exp}
PROPDMGEXP.by.year <- tapply(good$PROPDMGEXP, good$YEAR, function(x) table(x), simplify=TRUE)
sapply(PROPDMGEXP.by.year, function(x) length(names(x)))

PROPDMGEXP.by.year[which.max(sapply(PROPDMGEXP.by.year, function(x) length(names(x))))]

CROPDMGEXP.by.year <- tapply(good$CROPDMGEXP, good$YEAR, function(x) table(x), simplify=TRUE)
sapply(CROPDMGEXP.by.year, function(x) length(names(x)))

CROPDMGEXP.by.year[which.max(sapply(CROPDMGEXP.by.year, function(x) length(names(x))))]
```

We corrected their values according to the following interpretation:
+ The exponents are considered for the base of 10 as the base.
+ Missing values (NA) or empty strings are considered as $10^0=1$.
+ `H`, `K`, `M`, `B` are interpreted as: _hecto_ (=2), _kilo_ (=3), _mega_ (=6), _billion_ (=9).
+ Numeric exponents are interpreted as its corresponding power of 10 as the base.
+ The rest of effectively undefined exponents are also interpreted to be spurious and set to 0.
After these conversions the `PROPDMGEXP` and `CROPDMGEXP` variable are changed to _numeric_ class.

```{r reset-exp}
good$PROPDMGEXP[good$PROPDMGEXP == "?" | good$PROPDMGEXP == "+" | 
                good$PROPDMGEXP == "-" | good$PROPDMGEXP == ""] <- 0
good$PROPDMGEXP[good$PROPDMGEXP == "B"] <- 9
good$PROPDMGEXP[good$PROPDMGEXP == "M"] <- 6
good$PROPDMGEXP[good$PROPDMGEXP == "K"] <- 3
good$PROPDMGEXP[good$PROPDMGEXP == "H"] <- 2

good$CROPDMGEXP[good$CROPDMGEXP == "?" | good$CROPDMGEXP == "+" | 
                good$CROPDMGEXP == "-" | good$CROPDMGEXP == ""] <- 0
good$CROPDMGEXP[good$CROPDMGEXP == "B"] <- 9
good$CROPDMGEXP[good$CROPDMGEXP == "M"] <- 6
good$CROPDMGEXP[good$CROPDMGEXP == "K"] <- 3
good$CROPDMGEXP[good$CROPDMGEXP == "H"] <- 2

good$PROPDMGEXP <- as.numeric(good$PROPDMGEXP)
good$CROPDMGEXP <- as.numeric(good$CROPDMGEXP)
```

```{r}
str(good)
```


<hr class="thin_separator">
<a name="PERIOD_LIMIT"></a>


## The choice of limiting the analysis to recent years (>=1996)

After reviewing and cleaning the data, we choose to limit our analysis to the data since 1996-01-01.
There are two main motivations for this choice:
+ As noted above, before 1996 the NCDC database only tracked three event types _"Tornado"_",
_"Thunderstorm Wind"_" and _"Hail"_.  While it would certainly be interesting to perform an 
analysis of the longer time baseline for these three categories, here we are mostly interested
in assessing the impact of storm events from a broader, more inclusive perspective.
+ Imposing this cut yields a significantly _cleaner_ data set, for instance with respect to the 
`DMGEXP` and `EVTYPE` variables (but also the various date/time variables.)

```{r subset-recent, cache=TRUE}
# subsetting 'good' to 'recent' taking only events since 1996
recent <- subset(good, BGN_DATE > as.POSIXlt("1996-01-01"))
row.names(recent) <- NULL
```

```{r}
str(recent)
```

### Computing actual values for property and crop damages

Final step of the data processing is the computation of actual straight US Dollar values
for damages combining the `DMG` and `DMGEXP` variables. 

**NOTE**: before we compute them, we make an _ad hoc_ fix to the value of `PROPDMGEXP` for
one specific entry, a flood in Napa in 2005.  More details about this can be found in the Appendix.

```{r save-napa-data, cache=TRUE, echo=FALSE}
tmp.PropDamage <- recent$PROPDMG * 10^recent$PROPDMGEXP
```
```{r compute-damages, cache=TRUE}
recent$PROPDMGEXP[which.max(recent$PropDamage)] <- 6
recent$PropDamage <- recent$PROPDMG * 10^recent$PROPDMGEXP
recent$CropDamage <- recent$CROPDMG * 10^recent$CROPDMGEXP
str(recent)
```


<hr class="thin_separator">
<a name="RESULTS"></a>

## Results

We aim to answer two questions concerning the impact of different type of events across
the United States:

1. which types of events are most harmful with respect to population health?
2. which types of events have the greatest economic consequences?


### Further data manipulation 

We work with a data-frame comprising a subset of variables and summarize the data for human 
and economic impact in two final data-frames: `human` and `economic`.
In each of them, we aggregate data by `EVTYPE` and compute:

* number of events of that type.
* number of events with given _impact_ (that is fatalities, injuries, property and crop damages, all counted separately).
* fraction (%) of events with _impact_.
* cumulative number of _impact_ (e.g. fatalities, or Dollar-amount damage).
* average number of _impact_ (computed considering only the number of events with `>0` _impact_).

We keep only `EVTYPE` with at least 100 occurrences.

```{r create-lean-df}
lean <- recent[c(1,2,4,7,8,9,14:16)]
human <- ddply(lean, .(EVTYPE), summarize, 
               N.tot = length(FATALITIES), 
               N.with.F = length(FATALITIES[FATALITIES>0]), 
               pct.with.F = length(FATALITIES[FATALITIES>0])/length(FATALITIES)*100.,
               F.tot = sum(FATALITIES), 
               F.avrg = sum(FATALITIES)/length(FATALITIES[FATALITIES>0]), 
               N.with.I = length(INJURIES[INJURIES>0]), 
               pct.with.I = length(INJURIES[INJURIES>0])/length(FATALITIES)*100., 
               I.tot = sum(INJURIES), 
               I.avrg = sum(INJURIES)/length(INJURIES[INJURIES>0]), 
               flag =  (sum(FATALITIES) + sum(INJURIES))>0 ) 
human <- subset(human, N.tot >100)

economic <- ddply(lean, .(EVTYPE), summarize, 
                  N.tot = length(PropDamage), 
                  N.with.PrDmg = length(PropDamage[PropDamage>0]),
                  pct.with.PrDmg = length(PropDamage[PropDamage>0])/length(PropDamage)*100., 
                  PrDmg.tot = sum(PropDamage), 
                  PrDmg.avrg = sum(PropDamage)/length(PropDamage[PropDamage>0]),
                  N.with.CrDmg = length(CropDamage[CropDamage>0]),
                  pct.with.CrDmg = length(CropDamage[CropDamage>0])/length(PropDamage)*100., 
                  CrDmg.tot = sum(CropDamage), 
                  CrDmg.avrg = sum(CropDamage)/length(CropDamage[CropDamage>0]),
                  flag =  (sum(PropDamage) + sum(CropDamage))>0 ) 
economic <- subset(economic, N.tot >100)
```

```{r}
str(human)

str(economic)
```

With the data summarized we can easily slice them in different ways.   
In the next two sections we review the evidence emerging from the data collected and
organized as described so far.


### Population Health: injuries and fatalities

#### Fatalities 

The first table shows the top ten event types by total cumulative number of fatalities.

```{r table-fatalities-1}
print(human[order(human$F.tot, decreasing=TRUE),][1:10,c(1,5,2,3,4)], quote=FALSE, row.names=FALSE)
```

This instead ranks event types by the fraction (%) of events that caused fatalities.
```{r table-fatalities-2}
print(human[order(human$pct.with.F, decreasing=TRUE),][1:10,c(1,4,2,3,5)], quote=FALSE, row.names=FALSE)
```

#### Injuries

Injuries data can be looked at in the same two ways, ranked by total number of injuries and 
fraction that caused injuries.

```{r table-injuries-1}
print(human[order(human$I.tot, decreasing=TRUE),][1:10,c(1,9,2,7,8)], quote=FALSE, row.names=FALSE)
```

```{r table-injuries-2}
print(human[order(human$pct.with.I, decreasing=TRUE),][1:10,c(1,8,2,7,9)], quote=FALSE, row.names=FALSE)
```

```{r prepare-plot-hum}
hum20ft <- human[order(human$F.tot, decreasing=TRUE),][1:20,]
hum20fp <- human[order(human$pct.with.F, decreasing=TRUE),][1:20,]
hum20it <- human[order(human$I.tot, decreasing=TRUE),][1:20,]
hum20ip <- human[order(human$pct.with.I, decreasing=TRUE),][1:20,]
```

```{r plot-hum, fig.width=8.0, fig.height=10}
ph.ft <- ggplot(data=hum20ft, aes(x=EVTYPE)) + theme_bw()
ph.fp <- ggplot(data=hum20fp, aes(x=EVTYPE)) + theme_bw()

hplot.f1 <- ph.ft + geom_bar(stat="identity", aes(y=F.tot), fill="red2") + 
    theme(axis.text.x = element_text(angle = 45, hjust = 1)) + theme(legend.position="none") + 
    xlab("Event Type") + ylab("Fatalities Count") + 
    ggtitle("Events Causing Fatalities")

hplot.f2 <- ph.fp + geom_bar(stat="identity", aes(y=pct.with.F), fill="red2") + 
    theme(axis.text.x = element_text(angle = 45, hjust = 1)) + theme(legend.position="none") + 
    xlab("Event Type") + ylab("Percentage") + ylim(c(-5,75)) +
    ggtitle("Fraction of Events Causing Fatalities")

grid.draw(rbind(ggplotGrob(hplot.f1), ggplotGrob(hplot.f2),  size="last"))
```


<hr class="thin_separator">

### Economic impact: property, agricultural and total damages

As in the previous section, we report events ranked by total damage and by fraction of events 
causing a recorded amount of damages.

#### Property

```{r table-prop1}
print(economic[order(economic$pct.with.PrDmg, decreasing=TRUE),][1:10,c(1,4,2,3,5)], quote=FALSE, row.names=FALSE)
```
```{r table-prop2}
print(economic[order(economic$PrDmg.tot, decreasing=TRUE),][1:10,c(1,5,2,3,4)], quote=FALSE, row.names=FALSE)
```

#### Agricultural

```{r table-crop1}
print(economic[order(economic$CrDmg.tot, decreasing=TRUE),][1:10,c(1,9,2,7,8)], quote=FALSE, row.names=FALSE)
```
```{r table-crop2}
print(economic[order(economic$pct.with.CrDmg, decreasing=TRUE),][1:10,c(1,8,2,7,9)], quote=FALSE, row.names=FALSE)
```

```{r prepare-plot-econ}
eco20plot <- economic[order(economic$PrDmg.tot, decreasing=TRUE),][1:20,]
order.pdt <- order(eco20plot$PrDmg.tot, decreasing=TRUE)
order.pdp <- order(eco20plot$pct.with.PrDmg, decreasing=TRUE)
order.cdt <- order(eco20plot$CrDmg.tot, decreasing=TRUE)
order.cdp <- order(eco20plot$pct.with.CrDmg, decreasing=TRUE)
eco20plot$PrDmg.tot.rank <- rep(0,20) ;  eco20plot$PrDmg.tot.rank[order.pdt] <- 1:20
eco20plot$PrDmg.pct.rank <- rep(0,20) ;  eco20plot$PrDmg.pct.rank[order.pdp] <- 1:20
eco20plot$CrDmg.tot.rank <- rep(0,20) ;  eco20plot$CrDmg.tot.rank[order.cdt] <- 1:20
eco20plot$CrDmg.pct.rank <- rep(0,20) ;  eco20plot$CrDmg.pct.rank[order.cdp] <- 1:20
eco20plot$PrDmg.tot.rank <- factor(eco20plot$PrDmg.tot.rank, levels=1:20, ordered=TRUE)
eco20plot$PrDmg.pct.rank <- factor(eco20plot$PrDmg.pct.rank, levels=1:20, ordered=TRUE)
eco20plot$CrDmg.tot.rank <- factor(eco20plot$CrDmg.tot.rank, levels=1:20, ordered=TRUE)
eco20plot$CrDmg.pct.rank <- factor(eco20plot$CrDmg.pct.rank, levels=1:20, ordered=TRUE)
```


<hr class="thin_separator">

```{r plot-econ, fig.width=8.0, fig.height=10}
p <- ggplot(data=eco20plot, aes(x=EVTYPE)) + theme_bw()

ecoplot.p1 <- p + geom_bar(stat="identity", aes(y=PrDmg.tot/1.0e6, fill=PrDmg.tot.rank)) + 
    theme(axis.text.x = element_text(angle = 45, hjust = 1)) + theme(legend.position="none") + 
    scale_y_log10() + coord_cartesian(ylim=c(1e1, 3e5)) +
    xlab("") + ylab("Damages (Million of Dollars)") +
    ggtitle("Total Property Damages by Event Type") 

ecoplot.p2 <- p + geom_bar(stat="identity", aes(y=pct.with.PrDmg, fill=PrDmg.pct.rank)) + 
    theme(axis.text.x = element_text(angle = 45, hjust = 1)) + theme(legend.position="none") + 
    xlab("Event Type") + ylab("Percentage") + ylim(c(-5,105)) +
    ggtitle("Fraction of Events Causing Damages")

grid.draw(rbind(ggplotGrob(ecoplot.p1), ggplotGrob(ecoplot.p2),  size="last"))
```

Next, we can apply the same logic to identify the events which cause the
greatest economic impacts. First, we’ll split the dataframe in order to sum the
total property damages and crop damages per event type and then we’ll sort the
data and extract the top 10 events which cause the most economic damages to
properties and to crops.


<hr class="separator">
<a name="APPENDIX"></a>

# APPENDIX

## The _"anomalous"_ 2005 Napa Valley Flood

Going by the "raw" data, the event that caused the highest property damage, was a flood 
event in Napa, California, at the end of 2005, reported at over 100 Billion USD. 
However, the `REMARKS` entry in the database itsels raises doubts about this figure, and a 
USGS report assessing the impact of the late 2005 storms confirms that.  
Our best guess is that the value of the `EXP` parameter for this event should been `M` instead of `M` 
as this would bring the damages amount in line with the narrative remarks and USGS report.
We therefore adjusted accordingly the `PROPDMGEXP` for this specific event.

Here is the original data:

```{r explain-napa, cache=TRUE}
recent[which.max(tmp.PropDamage),]
data$REMARKS[data$REFNUM == recent$REFNUM[which.max(tmp.PropDamage)]]
```

Now, the single event which resulted in highest property damage is storm surge
caused by Hurricane Katrina in Lousiana, resulting in over 31 Billion.


## _EVTYPE_ regularization

This is the source of the `evt_regularization.R` script:
```{r echo = FALSE, cache = FALSE}
read_chunk("./scripts/evtype_regularization.R")
```

```{r eval = FALSE, cache = FALSE}
<<regularization>>
```

## R Session Info

```{r R_session_info}
sessionInfo()
```
---
