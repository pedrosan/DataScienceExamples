<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />

<meta name="author" content="Giovanni Fossati" />


<title>Building a Text Prediction Algorithm</title>

<script src="../../common/libs/jquery-1.11.0/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="../../common/libs/bootstrap-3.3.1/css/cerulean.min.css" rel="stylesheet" />
<script src="../../common/libs/bootstrap-3.3.1/js/bootstrap.min.js"></script>
<script src="../../common/libs/bootstrap-3.3.1/shim/html5shiv.min.js"></script>
<script src="../../common/libs/bootstrap-3.3.1/shim/respond.min.js"></script>

<script src="../../common/js/toggle_GF.js"></script>


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; background-color: #f8f8f8; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
pre, code { background-color: #f8f8f8; }
code > span.kw { color: #204a87; font-weight: bold; }
code > span.dt { color: #204a87; }
code > span.dv { color: #0000cf; }
code > span.bn { color: #0000cf; }
code > span.fl { color: #0000cf; }
code > span.ch { color: #4e9a06; }
code > span.st { color: #4e9a06; }
code > span.co { color: #8f5902; font-style: italic; }
code > span.ot { color: #8f5902; }
code > span.al { color: #ef2929; }
code > span.fu { color: #000000; }
code > span.er { font-weight: bold; }
</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>


<link rel="stylesheet" href="../../common/css/gf_xnew_nolines.css" type="text/css" />

</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img { 
  max-width:100%; 
  height: auto; 
}
</style>
<div class="container-fluid main-container">


<div id="header">
<h1 class="title">Building a Text Prediction Algorithm</h1>
<h3 class="subtitle"><em>Exploratory Analysis and Thoughts about a Prediction Strategy</em></h3>
<h4 class="author"><em>Giovanni Fossati</em></h4>
</div>


<p><a name="SUMMARY"></a></p>
<div id="summary" class="section level1">
<h1>SUMMARY</h1>
<p>In this report I briefly illustrate the exploratory analysis performed on a three datasets, comprising text from blogs, news and tweets.</p>
<p>The ultimate goal is to produce a <em>light</em> application able to predict text (words) given some preceding text, mimicking the predictive typing feature of modern software keyboard of portable devices.</p>
<p>As a playground a fairly substantial dataset was made available, comprising text from various heterogenous sources (blogs, news, twitter). These datasets are the foundation for developing an understanding of <em>language processing</em> and in turn devise a strategy for achieving the goal, and perhaps more importantly (in practice) they constitute our training and testing datasets.</p>
<p>I decided to invest a significant amount of time to explore the data, and delved (too) deeply into data cleaning, assuming that this effort will pay off by making any algorithm more robust.</p>
<p>At this stage in the project I will mostly review my exploratory analysis of the data, and outline my current thought about the strategy for developing the algorithm for the text-predicting application.</p>
<p><strong>Performance issues</strong>: it is worth mentioning that one of the main challenges has been dealing smartly with the computational load, that turned out to be a serious limiting factor, even on a powerful workstation.<br />I did not use the suggested <code>tm</code> suite and relied instead heavily on <code>perl</code> and in <code>R</code> mainly <code>dplyr</code>, <code>NLP</code> and <code>RWeka</code>.</p>
<div id="current-thoughts-about-predictive-algorithm-strategy" class="section level3">
<h3>Current Thoughts About Predictive Algorithm Strategy</h3>
<p>My current thoughts, very much in flux, about the strategy are that a <em>n-grams</em> based approach would be the most effective.<br />In particular, I am leaning towards a <em>weighted combination</em> of <em>2- 3- 4- 5-grams</em> (linear interpolation), perhaps <em>assisted</em> by some additional information drawn from an analysis of the association of words in sentences or their distance within it.</p>
<p>An important issue that I have not yet had a chance to ponder sufficiently include the handling of <em>‚Äúzeros‚Äù</em>, <em>i.e.</em> words not included in the dictionary of the training set or, more importantly with a <em>n-grams</em> approach words that are not seen following a given <em>(n-1) gram</em>. In practice, based on my readings, this problem is tackled with some form of <em>smoothing</em>, that is assigning a probability to the <em>‚Äúzeros‚Äù</em> (and in turn re-allocating some mass probability away from the observed <em>n-grams</em>).<br />I have not yet had a chance to explore the feasibility and effectiveness of methods like <em>Good-Turing</em> or <em>Stupid Backoff</em>.</p>
<p><a name="CONTENT"></a></p>
</div>
<div id="content" class="section level2">
<h2>CONTENT</h2>
<p>The report is organized in the following sections:</p>
<ul>
<li><a href="#SUMMARY">EXECUTIVE SUMMARY</a></li>
<li><a href="#PRELIMINARIES">PRELIMINARIES</a>
<ul>
<li><a href="#PREPROCESSING">Preprocessing (before loading into R)</a>
<ul>
<li><a href="#weird_characters">Removal of weird characters</a></li>
<li><a href="#Homogeneization">Homogeneization of characters</a></li>
<li><a href="#Contractions">Contractions, profanities, emoticons, hashtags, etc‚Ä¶</a></li>
<li><a href="#exclude_short_rows">Excluding rows with less than 6 words</a></li>
</ul></li>
</ul></li>
<li><a href="#LOADING">MOVING TO R</a>
<ul>
<li><a href="#LOADING_THE_DATA">Loading the Data</a></li>
<li><a href="#FURTHER_CLEANING">Further Data Cleaning in R</a>
<ul>
<li><a href="#text_transformations">Text transformations</a></li>
<li><a href="#subsetting">Subsetting of the data</a></li>
</ul></li>
</ul></li>
<li><a href="#ANALYSIS">ANALYSIS</a>
<ul>
<li><a href="#ANALYSIS-STEP1">Step 1 : Sentence Annotation</a></li>
<li><a href="#ANALYSIS-STEP2">Step 2 : <em>n-grams</em> Tokenization</a>
<ul>
<li><a href="#ngrams-cleaning_and_reclassification">Cleaning and <em>re-classification</em> of n-grams</a></li>
<li><a href="#ngrams_tables">A look at the n-grams</a></li>
<li><a href="#ngrams_plots">Some Summary Plots for 4-grams</a>
<ul>
<li><a href="#top30_mixed">Top-30 all mixed</a></li>
<li><a href="#top20_separate">Top-20 by data source</a></li>
</ul></li>
</ul></li>
</ul></li>
<li><a href="#APPENDIX">APPENDIX</a>
<ul>
<li><a href="#APPENDIX-my_functions">User Defined Functions</a></li>
<li><a href="#APPENDIX-regularize">Summary of regularization done with <em>perl scripts</em></a></li>
<li><a href="#APPENDIX-post_sentence_cleaning">Summary of post-sentence-tokenization cleaning (with <em>perl scripts</em>)</a></li>
<li><a href="#APPENDIX-tokenization_issue">Mysterious issue with <code>NGramTokenizer</code></a></li>
</ul></li>
</ul>
<hr class="thin_separator">
<p><a name="PRELIMINARIES"></a></p>
</div>
</div>
<div id="preliminaries" class="section level1">
<h1>PRELIMINARIES</h1>
<p><a href="#TOP">Back to the Top</a></p>
<p>Libraries needed for data processing and plotting:</p>
<button class="toggle_code">
show code
</button>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co">#-----------------------------</span>
<span class="co"># NLP</span>
<span class="kw">library</span>(<span class="st">&quot;tm&quot;</span>)
<span class="kw">library</span>(<span class="st">&quot;SnowballC&quot;</span>)
<span class="kw">library</span>(<span class="st">&quot;openNLP&quot;</span>)
<span class="kw">library</span>(<span class="st">&quot;NLP&quot;</span>)

<span class="co"># To help java fail less :-(</span>
<span class="kw">options</span>( <span class="dt">java.parameters =</span> <span class="st">&quot;-Xmx6g&quot;</span>)
<span class="kw">library</span>(<span class="st">&quot;RWeka&quot;</span>)   <span class="co"># [NGramTokenizer], [Weka_control]</span>

<span class="co">#-----------------------------</span>
<span class="co"># general</span>
<span class="kw">library</span>(<span class="st">&quot;dplyr&quot;</span>)
<span class="kw">library</span>(<span class="st">&quot;magrittr&quot;</span>)
<span class="kw">library</span>(<span class="st">&quot;devtools&quot;</span>)

<span class="kw">library</span>(<span class="st">&quot;ggplot2&quot;</span>)
<span class="kw">library</span>(<span class="st">&quot;gridExtra&quot;</span>)
<span class="co"># library(&quot;RColorBrewer&quot;)</span>

<span class="kw">library</span>(<span class="st">&quot;pander&quot;</span>)

<span class="co">#-----------------------------</span>
<span class="co"># my functions</span>
<span class="kw">source</span>(<span class="st">&quot;./scripts/my_functions.R&quot;</span>)
<span class="co">#-----------------------------</span></code></pre>
<div id="the-data" class="section level2">
<h2>THE DATA</h2>
<p><a href="#CONTENT">Back to TOC</a></p>
<p>The datasets are read-in separately into character vectors, using a user-defined compact function (<code>readByLine()</code>) (see <a href="#APPENDIX-my_functions"><strong>Appendix</strong></a> for the short source).</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># NOT EVALUATED because too computationally heavy (loading saved products)</span>
in.blogs.ORIG &lt;-<span class="st"> </span><span class="kw">readByLine</span>(<span class="st">&quot;./data/en_US.blogs.ORIGINAL.txt.gz&quot;</span>, <span class="dt">check_nl =</span> <span class="ot">FALSE</span>, <span class="dt">skipNul =</span> <span class="ot">TRUE</span>)
in.news.ORIG &lt;-<span class="st"> </span><span class="kw">readByLine</span>(<span class="st">&quot;./data/en_US.news.ORIGINAL.txt.gz&quot;</span>, <span class="dt">check_nl =</span> <span class="ot">FALSE</span>, <span class="dt">skipNul =</span> <span class="ot">TRUE</span>)
in.twitter.ORIG &lt;-<span class="st"> </span><span class="kw">readByLine</span>(<span class="st">&quot;./data/en_US.twitter.ORIGINAL.txt.gz&quot;</span>, <span class="dt">check_nl =</span> <span class="ot">FALSE</span>, <span class="dt">skipNul =</span> <span class="ot">TRUE</span>)</code></pre>
<p>Basic statistics of the three datasets in their original form:</p>
<button class="toggle_code">
show code
</button>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># NOT EVALUATED</span>
stats.blogs   &lt;-<span class="st"> </span><span class="kw">as.numeric</span>(<span class="kw">system</span>(<span class="st">&quot;gzip -dc ./data/en_US.blogs.ORIGINAL.txt.gz | wc | awk &#39;{print $1; print $2; print $3}&#39;&quot;</span>, <span class="dt">intern =</span> <span class="ot">TRUE</span>))
stats.news    &lt;-<span class="st"> </span><span class="kw">as.numeric</span>(<span class="kw">system</span>(<span class="st">&quot;gzip -dc ./data/en_US.news.ORIGINAL.txt.gz | wc | awk &#39;{print $1; print $2; print $3}&#39;&quot;</span>, <span class="dt">intern =</span> <span class="ot">TRUE</span>))
stats.twitter &lt;-<span class="st"> </span><span class="kw">as.numeric</span>(<span class="kw">system</span>(<span class="st">&quot;gzip -dc ./data/en_US.twitter.ORIGINAL.txt.gz | wc | awk &#39;{print $1; print $2; print $3}&#39;&quot;</span>, <span class="dt">intern =</span> <span class="ot">TRUE</span>))

stats.ORIG.df &lt;-<span class="st"> </span><span class="kw">data.frame</span>( <span class="dt">blogs =</span> stats.blogs, <span class="dt">news =</span> stats.news, <span class="dt">twitter =</span> stats.twitter, 
                             <span class="dt">row.names =</span> <span class="kw">c</span>(<span class="st">&quot;lines&quot;</span>, <span class="st">&quot;words&quot;</span>, <span class="st">&quot;characters&quot;</span>), <span class="dt">stringsAsFactors =</span> <span class="ot">FALSE</span>)
<span class="kw">saveRDS</span>(stats.ORIG.df, <span class="st">&quot;data/stats_ORIGINAL.RDS&quot;</span>)</code></pre>
<table>
<thead>
<tr class="header">
<th align="left"></th>
<th align="right">blogs</th>
<th align="right">news</th>
<th align="right">twitter</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">lines</td>
<td align="right">899288</td>
<td align="right">1010242</td>
<td align="right">2360148</td>
</tr>
<tr class="even">
<td align="left">words</td>
<td align="right">37334114</td>
<td align="right">34365936</td>
<td align="right">30359804</td>
</tr>
<tr class="odd">
<td align="left">characters</td>
<td align="right">210160014</td>
<td align="right">205811889</td>
<td align="right">167105338</td>
</tr>
</tbody>
</table>
<p><a name="PREPROCESSING"></a></p>
</div>
<div id="preprocessing-before-loading-into-r" class="section level2">
<h2>PREPROCESSING (before loading into R)</h2>
<p><a href="#CONTENT">Back to TOC</a></p>
<p>After a quick review of the data with various R functions and packages, I decided to perform some cleaning of the text with standard <em>Linux</em> command line tools, mostly <strong>perl</strong> scripts. Broadly speaking I performed three categories of transformations:</p>
<ul>
<li>Removal of ‚Äúweird‚Äù characters.</li>
<li>Homogeneization of characters.</li>
<li>Regularization of text, <em>e.g.</em> dealing with hashtags, profanities, emoticons, acronyms, numbers, ‚Äúmessy punctuation‚Äù</li>
</ul>
<p>which I will describe below.</p>
<p><a name="weird_characters"></a></p>
<div id="weird-characters" class="section level3">
<h3><em>Weird</em> Characters</h3>
<p>The first task was to analyze the mix of invidual characters present in the three datasets with the goal of doing some homogeneization and tidying up of non-alphanumeric characters, such as quotes that can come in different forms.</p>
<p>The used method is not elegant, but effective enough, relying on a simple perl command substituting a series of <em>non-odd</em> characters with spaces, thus leaving a stream of <em>odd</em> characters subsequently parsed and cleaned to produce a list of <em>odd</em> characters sorted by their count.</p>
<pre class="sourceCode bash"><code class="sourceCode bash"><span class="kw">perl</span> -pe <span class="st">&#39;s|[\d\w\$\,\.\!\?\(\);:\/\\\-=&amp;%#_\~&lt;&gt;]||g; s|\s||g; s|[\^@&quot;\+\*\[\]]||g;&#39;</span> <span class="kw">|</span> <span class="kw">\</span>
          <span class="kw">perl</span> -pe <span class="st">&quot;s/\&#39;//g;&quot;</span> <span class="kw">|</span> <span class="kw">\</span>
          <span class="kw">egrep</span> -v <span class="st">&#39;^$&#39;</span> <span class="kw">|</span> <span class="kw">\</span>
          <span class="kw">split_to_singles.awk</span> <span class="kw">|</span> <span class="kw">\</span>
          <span class="kw">sort</span> -k 1 <span class="kw">|</span> <span class="kw">uniq</span> -c <span class="kw">|</span> <span class="kw">sort</span> -k 1nr

<span class="co"># split_to_singles.awk is a short awk script not worth including here (it&#39;s on GitHub)</span></code></pre>
<p>The number of unique <em>odd</em> characters found in each dataset are 2159 for blogs, 310 for news, 2087 for twitter.</p>
<p>The following is the census of <em>odd characters</em> appearing more than 500 times in each of the datasets (the full sorted lists are available on the GitHub repo in the data directory).</p>
<pre class="sourceCode r"><code class="sourceCode r">   blogs           news              twitter
-----------<span class="st">      </span>----------<span class="st">         </span>------------------------
<span class="st"> </span><span class="dv">387317</span> [‚Äô]      <span class="dv">102911</span> [‚Äô]         <span class="dv">27440</span> [‚Äú]        <span class="dv">726</span> [¬ª]
 <span class="dv">109154</span> [‚Äù]       <span class="dv">48115</span> [‚Äî]         <span class="dv">26895</span> [‚Äù]        <span class="dv">718</span> [¬´]
 <span class="dv">108769</span> [‚Äú]       <span class="dv">47090</span> [‚Äú]         <span class="dv">11419</span> [‚Äô]        <span class="dv">715</span> [üòî]
  <span class="dv">50176</span> [‚Äì]       <span class="dv">43992</span> [‚Äù]          <span class="dv">5746</span> [‚ô•]        <span class="dv">686</span> [üòâ]
  <span class="dv">41129</span> [‚Ä¶]        <span class="dv">8650</span> [‚Äì]          <span class="dv">5241</span> [‚Ä¶]        <span class="dv">680</span> [üò≥]
  <span class="dv">23836</span> [‚Äò]        <span class="dv">6991</span> [√∏]          <span class="dv">3838</span> [|]        <span class="dv">639</span> [{]
  <span class="dv">18757</span> [‚Äî]        <span class="dv">6723</span> [¬ì]          <span class="dv">2353</span> [‚ù§]        <span class="dv">617</span> [‚Ä¢]
   <span class="dv">3963</span> [√©]        <span class="dv">6544</span> [¬î]          <span class="dv">2314</span> [‚Äì]        <span class="dv">593</span> [‚Äò]
   <span class="dv">2668</span> [¬£]        <span class="dv">6267</span> [¬í]          <span class="dv">1799</span> [‚Äî]        <span class="dv">578</span> [ÔøΩ]
   <span class="dv">1301</span> [‚Ä≤]        <span class="dv">4898</span> [‚Äò]          <span class="dv">1333</span> [üòä]        <span class="dv">561</span> [üíú]
    <span class="dv">914</span> [¬¥]        <span class="dv">3641</span> [¬ñ]          <span class="dv">1211</span> [üëç]        <span class="dv">560</span> [üòÉ]
    <span class="dv">755</span> [‚Ä≥]        <span class="dv">3319</span> [√©]          <span class="dv">1149</span> [üòÇ]        <span class="dv">544</span> [üòè]
    <span class="dv">643</span> [‚Ç¨]        <span class="dv">3062</span> [‚Ä¶]           <span class="dv">977</span> [√©]        <span class="dv">506</span> [‚òÄ]
    <span class="dv">624</span> [ƒÅ]        <span class="dv">2056</span> [¬ó]           <span class="dv">963</span> [üòÅ]        <span class="dv">503</span> [üòú]
    <span class="dv">605</span> [¬Ω]        <span class="dv">1408</span> [¬ï]           <span class="dv">955</span> [‚ò∫]
    <span class="dv">598</span> [√°]        <span class="dv">1152</span> [ÔøΩ]           <span class="dv">926</span> [üòí]
    <span class="dv">582</span> [√∂]         <span class="dv">971</span> [‚Ä¢]           <span class="dv">802</span> [<span class="st">`</span><span class="dt">]</span>
<span class="dt">    555 [√®]         837 [¬Ω]           758 [üòç]</span>
<span class="dt">    518 [¬∞]         711 [</span><span class="st">`</span>]           <span class="dv">751</span> [üòò]
                    <span class="dv">537</span> [√±]           <span class="dv">741</span> [}]</code></pre>
<p><a name="Homogeneization"></a></p>
</div>
<div id="homogeneization-of-characters" class="section level3">
<h3>Homogeneization of Characters</h3>
<p><a href="#CONTENT">Back to TOC</a></p>
<p>For this preliminary stage I decided to not worry about accented letters, and characters from non-latin alphabet (<em>e.g.</em> asian, emoticons), but I thought it would be helpful to standardize a small set of very frequent characters, whose ‚Äúmeaning‚Äù is substantially equivalent</p>
<table>
<thead>
<tr class="header">
<th align="left"></th>
<th align="center"></th>
<th align="right">blogs</th>
<th align="right">news</th>
<th align="right">twitter</th>
<th align="right">TOTAL</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">quotes</td>
<td align="center">[‚Äò]</td>
<td align="right">23836</td>
<td align="right">4898</td>
<td align="right">593</td>
<td align="right">29327</td>
</tr>
<tr class="even">
<td align="left"></td>
<td align="center">[‚Äô]</td>
<td align="right">387317</td>
<td align="right">102911</td>
<td align="right">11419</td>
<td align="right">501647</td>
</tr>
<tr class="odd">
<td align="left"></td>
<td align="center">[‚Äú]</td>
<td align="right">108769</td>
<td align="right">47090</td>
<td align="right">27440</td>
<td align="right">183299</td>
</tr>
<tr class="even">
<td align="left"></td>
<td align="center">[‚Äù]</td>
<td align="right">109154</td>
<td align="right">43992</td>
<td align="right">26895</td>
<td align="right">180041</td>
</tr>
<tr class="odd">
<td align="left"></td>
<td align="center">[¬´]</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">718</td>
<td align="right">718</td>
</tr>
<tr class="even">
<td align="left"></td>
<td align="center">[¬ª]</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">726</td>
<td align="right">726</td>
</tr>
<tr class="odd">
<td align="left"></td>
<td align="center"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
</tr>
<tr class="even">
<td align="left">dashes</td>
<td align="center">[‚Äì]</td>
<td align="right">50176</td>
<td align="right">8650</td>
<td align="right">2314</td>
<td align="right">61140</td>
</tr>
<tr class="odd">
<td align="left"></td>
<td align="center">[‚Äî]</td>
<td align="right">48115</td>
<td align="right">18757</td>
<td align="right">1799</td>
<td align="right">68671</td>
</tr>
<tr class="even">
<td align="left"></td>
<td align="center"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
</tr>
<tr class="odd">
<td align="left">ellipsis</td>
<td align="center">[‚Ä¶]</td>
<td align="right">41129</td>
<td align="right">5241</td>
<td align="right">3062</td>
<td align="right">49432</td>
</tr>
</tbody>
</table>
<p><a name="Contractions"></a></p>
</div>
<div id="contractions-profanities-emoticons-hashtags-etc" class="section level3">
<h3>Contractions, Profanities, Emoticons, Hashtags, etc‚Ä¶</h3>
<p><a href="#CONTENT">Back to TOC</a></p>
<p>I have put a major effort into understanding the idiosyncrasies of the textual data, with the expectation that a deep cleaning would make a difference in the prediction context.</p>
<p>One example of what I have in mind is that transforming to categorical generic ‚Äútag‚Äù frequent ‚Äúitems‚Äù with a lot of variations but broadly similar meaning (e.g.¬†dates, money, possessive pronouns), could strengthen the predictive ability of any algorithm.</p>
<p>Most of the work was done with <code>perl</code> ‚Äúoffline‚Äù (can‚Äôt beat it for <code>regex</code> work).<br />To match the application input with the data on which the application is built, all operations were ported to <code>R</code> either directly or by relying on an external perl script. Among the main transformations applied to the text:</p>
<ul>
<li><strong>Contractions</strong> (<em>e.g.</em> don‚Äôt, isn‚Äôt, I‚Äôll): this seem to be more commonly regarded as stopword, hence removed. My take has been that they can provide meaning and it was worth preserving them, as well as they non-contracted counterparts. I homogeneized all of them in forms like ‚ÄúI_will‚Äù, ‚Äúdo_not‚Äù, with an underscore gluing them together.</li>
<li><strong>Profanity filtering</strong>: I based my cleaning on the ‚Äú7 dirt words‚Äù, and some words rooted on them.
<ul>
<li>To preserve their potential predictive value, I replace them with a tag <code>&lt;PROFANITY&gt;</code>.</li>
<li>User input is also filtered, but the information carried by a possible profanity can be used.</li>
</ul></li>
<li><strong>Emoticons</strong>: Recognized them with regex. Marked with a tag, <code>&lt;EMOJ&gt;</code>.</li>
</ul>
<p>Other transformations done on the text before loading the data into R:</p>
<ul>
<li><strong>Regularization/ Homogeneization of Characters</strong>
<ul>
<li>Mostly cleaning (not necessarily removing) <em>odd characters</em> e.g.¬†apostrophes, quotes, etc.</li>
<li>Sequences of characters: inline and End-Of-Line <em>ellipsis</em>, and other ‚Äúnon-sense‚Äù.</li>
<li>Substitution on ‚Äú|‚Äù that seem to be equivalent to end of sentences (i.e.¬†a period).</li>
<li>Substitution of <code>&lt;==/&lt;--</code> and <code>==&gt;/--&gt;</code> with <code>;</code>.</li>
<li>Cleaning sequences of <code>!</code> and <code>?</code>.</li>
</ul></li>
<li><strong>Hashtags</strong>: Recognized and replaced with a generic tag <code>HASHTAG</code></li>
<li><strong>Acronyms</strong>: limited to variations of <code>U.S.</code>, also replaced with a tag, <code>&lt;USA&gt;</code>.</li>
<li><strong>Number-related</strong>:
<ul>
<li>(likely) <strong>dollar amounts</strong> by the presence of <code>$</code>: marked with <code>&lt;MONEY&gt;</code> tag.</li>
<li><strong>dates</strong> (<em>e.g.¬†12/34/5678</em>): marked with <code>&lt;DATE&gt;</code> tag.</li>
<li><strong>hours</strong> (<em>e.g.¬†1:30 p.m.</em>): marked with <code>&lt;HOUR&gt;</code> tag.</li>
<li><em>percentages</em>: marked with <code>&lt;PERCENTAGE&gt;</code> tag.</li>
</ul></li>
<li><strong>Repeated Consecutive Characters</strong>: handled by type.
<ul>
<li><code>$</code> signs, assumed to stand for a money: replaced with tag <code>&lt;MONEY&gt;</code>.</li>
<li><code>*</code>, within words usually are disguised profanities: replaced with <code>&lt;PROFANITY&gt;</code> tag.</li>
<li><code>-</code>: context/surroundings dependent replacement with regular punctuation.</li>
<li>Some character sequences were entirely deleted: multiple <code>&lt;</code>, <code>&gt;</code>, <code>=</code>, <code>#</code>.</li>
</ul></li>
</ul>
<p>The dataset where cleaned with <strong>perl scripts</strong>, available on <a href="https://github.com/pedrosan/DataScienceExamples/tree/master/myTextPredictr/MilestoneReport/scripts">GitHub</a> (look for <code>regularize_text-pass[1-5].pl</code> and <code>remove_tags_content.pl</code> scripts).<br />A summary of what they do is listed in the <a href="#APPENDIX-regularize">Appendix</a></p>
<p><a name="exclude_short_rows"></a></p>
</div>
<div id="excluding-rows-with-less-than-6-words" class="section level3">
<h3>Excluding rows with less than 6 words</h3>
<p><a href="#CONTENT">Back to TOC</a></p>
<p>During my initial attempts it immediately emerged the problem of excessively short rows of text. In particular, because I decided to perform tokenization on individual sentences, not directly on individual rows, the tokenizer tripped and failed on empty ‚Äúsentences‚Äù resulting from short rows.</p>
<p>I have then decided to set a cutoff to the minimum acceptable length of rows. After some empirical testing and row-length analysis with command line tools I have set a threshold at <span class="math">\(\ge6\)</span> words.</p>
<hr class="thin_separator">
<p><a name="LOADING"></a></p>
</div>
</div>
</div>
<div id="moving-to-r" class="section level1">
<h1>MOVING TO R</h1>
<p><a name="LOADING_THE_DATA"></a></p>
<div id="loading-the-data" class="section level2">
<h2>Loading the data</h2>
<p><a href="#CONTENT">Back to TOC</a></p>
<p>The rest of the analysis presented here is based on the <em>cleaned</em> datasets resulting from the processing described in the previous sections.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># NOT EVALUATED because too computationally heavy (loading saved products)</span>
in.blogs.REG   &lt;-<span class="st"> </span><span class="kw">readByLine</span>(<span class="st">&quot;./data/blogs_REG.txt.gz&quot;</span>, <span class="dt">check_nl =</span> <span class="ot">FALSE</span>, <span class="dt">skipNul =</span> <span class="ot">TRUE</span>)
in.news.REG    &lt;-<span class="st"> </span><span class="kw">readByLine</span>(<span class="st">&quot;./data/news_REG.txt.gz&quot;</span>, <span class="dt">check_nl =</span> <span class="ot">FALSE</span>, <span class="dt">skipNul =</span> <span class="ot">TRUE</span>)
in.twitter.REG &lt;-<span class="st"> </span><span class="kw">readByLine</span>(<span class="st">&quot;./data/twitter_REG.txt.gz&quot;</span>, <span class="dt">check_nl =</span> <span class="ot">FALSE</span>, <span class="dt">skipNul =</span> <span class="ot">TRUE</span>)</code></pre>
<p>Basic statistics of the three datasets in their original form:</p>
<button class="toggle_code">
show code
</button>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># NOT EVALUATED</span>
stats.blogs   &lt;-<span class="st"> </span><span class="kw">as.numeric</span>(<span class="kw">system</span>(<span class="st">&quot;gzip -dc ./data/blogs_REG.txt.gz | wc | awk &#39;{print $1; print $2; print $3}&#39;&quot;</span>, <span class="dt">intern =</span> <span class="ot">TRUE</span>))
stats.news    &lt;-<span class="st"> </span><span class="kw">as.numeric</span>(<span class="kw">system</span>(<span class="st">&quot;gzip -dc ./data/news_REG.txt.gz | wc | awk &#39;{print $1; print $2; print $3}&#39;&quot;</span>, <span class="dt">intern =</span> <span class="ot">TRUE</span>))
stats.twitter &lt;-<span class="st"> </span><span class="kw">as.numeric</span>(<span class="kw">system</span>(<span class="st">&quot;gzip -dc ./data/twitter_REG.txt.gz | wc | awk &#39;{print $1; print $2; print $3}&#39;&quot;</span>, <span class="dt">intern =</span> <span class="ot">TRUE</span>))

stats.REG.df &lt;-<span class="st"> </span><span class="kw">data.frame</span>( <span class="dt">blogs =</span> stats.blogs, <span class="dt">news =</span> stats.news, <span class="dt">twitter =</span> stats.twitter, 
                             <span class="dt">row.names =</span> <span class="kw">c</span>(<span class="st">&quot;lines&quot;</span>, <span class="st">&quot;words&quot;</span>, <span class="st">&quot;characters&quot;</span>), <span class="dt">stringsAsFactors =</span> <span class="ot">FALSE</span>)
<span class="kw">saveRDS</span>(stats.REG.df, <span class="st">&quot;data/stats_REG.RDS&quot;</span>)</code></pre>
<p>After preprocessing we have the following stats:</p>
<table>
<thead>
<tr class="header">
<th align="left"></th>
<th align="right">blogs</th>
<th align="right">news</th>
<th align="right">twitter</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">lines</td>
<td align="right">793096</td>
<td align="right">957228</td>
<td align="right">2072644</td>
</tr>
<tr class="even">
<td align="left">words</td>
<td align="right">36606082</td>
<td align="right">33908035</td>
<td align="right">29072455</td>
</tr>
<tr class="odd">
<td align="left">characters</td>
<td align="right">207683984</td>
<td align="right">206471971</td>
<td align="right">162152503</td>
</tr>
</tbody>
</table>
<p><a name="FURTHER_CLEANING"></a></p>
</div>
<div id="further-data-cleaning-in-r" class="section level2">
<h2>Further Data Cleaning in R</h2>
<p><a href="#CONTENT">Back to TOC</a></p>
<p>There are some common, customary, operations performed on a text dataset before proceeding to analyze it.</p>
<ul>
<li><strong>Make text lowercase</strong>.</li>
<li><strong>Strip extra white spaces</strong>.</li>
<li><strong>Remove numbers</strong>.</li>
<li>Remove punctuation.</li>
<li>Remove <em>stopwords</em>.</li>
</ul>
<p>Given that the goal is to <strong>predict words in a typing context</strong> I think that removing <em>stopwords</em> does not make much sense.<br />Working with a text without <em>stopwords</em> may be useful if one wanted to use in the prediction algorithm some information about words‚Äô association in sentences, which may help improve meaningful discrimination between different <em>next word</em> possibilities ‚Äúproposed‚Äù by an algorithm based on <em>n-grams</em>.</p>
<p>Because of the context, I also do not think that removing punctuation would be wise, nor make sense.</p>
<p><a name="text_transformations"></a></p>
<div id="text-transformations" class="section level3">
<h3>Text transformations</h3>
<p><a href="#CONTENT">Back to TOC</a></p>
<p>The next step is applying the additional following three transformations:</p>
<ul>
<li>conversion to lower case.</li>
<li>removal of numbers.</li>
<li>removal of redundant white spaces.</li>
</ul>
<p>Done as follows (with big obligatory acknowledgement and thank you to Hadley Wickham and Stefan Bache for bringing us <em>the pipe</em> <code>%&gt;%</code>!).</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># NOT EVALUATED because too computationally heavy (loading saved products)</span>
in.blogs.REG &lt;-<span class="st"> </span><span class="kw">tolower</span>(in.blogs.REG) %&gt;%<span class="st"> </span><span class="kw">removeNumbers</span>() %&gt;%<span class="st"> </span><span class="kw">stripWhitespace</span>()
in.news.REG &lt;-<span class="st"> </span><span class="kw">tolower</span>(in.news.REG) %&gt;%<span class="st"> </span><span class="kw">removeNumbers</span>() %&gt;%<span class="st"> </span><span class="kw">stripWhitespace</span>()
in.twitter.REG &lt;-<span class="st"> </span><span class="kw">tolower</span>(in.twitter.REG) %&gt;%<span class="st"> </span><span class="kw">removeNumbers</span>() %&gt;%<span class="st"> </span><span class="kw">stripWhitespace</span>()

<span class="co"># re-uppercases TAGS</span>
in.blogs.REG &lt;-<span class="st"> </span><span class="kw">gsub</span>(<span class="st">&#39;&lt;(emoticon|hashtag|dollaramount|period|hour|profanity|usa|percentage|date|money|ass|space|decade|ordinal|number|telephonenumber|timeinterval)&gt;&#39;</span>, 
                  <span class="st">&#39;&lt;</span><span class="ch">\\</span><span class="st">U</span><span class="ch">\\</span><span class="st">1&gt;&#39;</span>, in.blogs.REG, <span class="dt">ignore.case =</span> <span class="ot">TRUE</span>, <span class="dt">perl =</span> <span class="ot">TRUE</span>)
in.news.REG &lt;-<span class="st"> </span><span class="kw">gsub</span>(<span class="st">&#39;&lt;(emoticon|hashtag|dollaramount|period|hour|profanity|usa|percentage|date|money|ass|space|decade|ordinal|number|telephonenumber|timeinterval)&gt;&#39;</span>, 
                  <span class="st">&#39;&lt;</span><span class="ch">\\</span><span class="st">U</span><span class="ch">\\</span><span class="st">1&gt;&#39;</span>, in.news.REG, <span class="dt">ignore.case =</span> <span class="ot">TRUE</span>, <span class="dt">perl =</span> <span class="ot">TRUE</span>)
in.twitter.REG &lt;-<span class="st"> </span><span class="kw">gsub</span>(<span class="st">&#39;&lt;(emoticon|hashtag|dollaramount|period|hour|profanity|usa|percentage|date|money|ass|space|decade|ordinal|number|telephonenumber|timeinterval)&gt;&#39;</span>, 
                  <span class="st">&#39;&lt;</span><span class="ch">\\</span><span class="st">U</span><span class="ch">\\</span><span class="st">1&gt;&#39;</span>, in.twitter.REG, <span class="dt">ignore.case =</span> <span class="ot">TRUE</span>, <span class="dt">perl =</span> <span class="ot">TRUE</span>)</code></pre>
<hr class="thin_separator">
<p><a name="ANALYSIS"></a></p>
</div>
</div>
</div>
<div id="analysis" class="section level1">
<h1>ANALYSIS</h1>
<p><a name="ANALYSIS-STEP1"></a></p>
<div id="step-1-sentence-annotation-r" class="section level2">
<h2>Step 1 : Sentence Annotation (R)</h2>
<p><a href="#CONTENT">Back to TOC</a></p>
<p>As noted, after some tests, I settled on an approach whereby <em>n-grams tokenization</em> is performed on separate individual sentences, instead of directly on individual rows as loaded from the dataset.</p>
<p>This is motivated by the fact that the <em>tokenizer</em> I have adopted because I found its performance to be more satisfactory, the <code>NGramTokenizer()</code> of the <strong>RWeka</strong> package, does not seem to interrupt its construction of <em>n-grams</em> at what are very likely sentence boundaries.</p>
<p>With <em>next word prediction</em> in mind, it makes a lot of sense to restrict <em>n-grams</em> to sequences of words <em>within the boundaries of a sentence</em>.</p>
<p>Therefore, after cleaning, transforming and filtering the data, the first real operation I perform is the annotation of sentences, for which I have been using the <strong>openNLP</strong> sentence annotator <code>Maxent_Sent_Token_Annotator()</code>, with its default settings, and the function <code>annotate()</code> from the <strong>NLP</strong> package.</p>
<pre class="sourceCode r"><code class="sourceCode r">sent_token_annotator &lt;-<span class="st"> </span><span class="kw">Maxent_Sent_Token_Annotator</span>()
sent_token_annotator
<span class="co"># An annotator inheriting from classes</span>
<span class="co">#   Simple_Sent_Token_Annotator Annotator</span>
<span class="co"># with description</span>
<span class="co">#   Computes sentence annotations using the Apache OpenNLP Maxent sentence detector</span>
<span class="co">#   employing the default model for language &#39;en&#39;.</span></code></pre>
<p>I want the data in the form of a <em>vector with individual sentences</em>, and so I opted for <code>sapply()</code> combined with a function wrapping the operations necessary to prepare a row of data for annotation, the annotation itself and finally return a vector of sentences.</p>
<pre class="sourceCode r"><code class="sourceCode r">find_sentences &lt;-<span class="st"> </span>function(x) {
    s &lt;-<span class="st"> </span><span class="kw">paste</span>(x, <span class="dt">collapse =</span> <span class="st">&quot; &quot;</span>) %&gt;%<span class="st"> </span><span class="kw">as.String</span>()
    a &lt;-<span class="st"> </span>NLP::<span class="kw">annotate</span>(s , sent_token_annotator) 
    <span class="kw">as.vector</span>(s[a])
}</code></pre>
<p>To work around not fully performance issues, the tokenization was done on subsets (chunk) of the data, comprising <span class="math">\(100000\)</span> lines. Done this way was much faster than forcing the tokenization over the The following code takes each dataset, and splits it into sentences chunk by chunk, writing out the sentences for each chunk to separate files, which were then concatenated outside of R.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># NOT EVALUATED because too computationally heavy (loading saved products)</span>
chunk_size &lt;-<span class="st"> </span><span class="dv">100000</span>

for( what in <span class="kw">c</span>(<span class="st">&quot;blogs&quot;</span>, <span class="st">&quot;news&quot;</span>, <span class="st">&quot;twitter&quot;</span>)) {
    
    data.what &lt;-<span class="st"> </span><span class="kw">get</span>(<span class="kw">paste0</span>(<span class="st">&quot;in.&quot;</span>, what, <span class="st">&quot;.REG&quot;</span>))
    len.what   &lt;-<span class="st"> </span><span class="kw">length</span>(data.what)
    <span class="kw">cat</span>(<span class="st">&quot; - length &quot;</span>, len.what, <span class="st">&quot;</span><span class="ch">\n</span><span class="st">&quot;</span>)
    
    n_chunks &lt;-<span class="st"> </span><span class="kw">floor</span>(len.what/chunk_size) +<span class="st"> </span><span class="dv">1</span>
    n1 &lt;-<span class="st"> </span>((<span class="dv">1</span>:n_chunks)-<span class="dv">1</span>)*chunk_size +<span class="st"> </span><span class="dv">1</span>
    n2 &lt;-<span class="st"> </span>(<span class="dv">1</span>:n_chunks)*chunk_size
    n2[n_chunks] &lt;-<span class="st"> </span>len.what
    Ns_by_chunk &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, n_chunks)
    <span class="kw">print</span>(n1)
    <span class="kw">print</span>(n2)
    
    names &lt;-<span class="st"> </span><span class="kw">paste</span>(what, <span class="st">&quot;sentences&quot;</span>, <span class="kw">sprintf</span>(<span class="st">&quot;%02d&quot;</span>, (<span class="dv">1</span>:n_chunks)), <span class="dt">sep =</span> <span class="st">&quot;.&quot;</span>)
    fnames &lt;-<span class="st"> </span><span class="kw">paste</span>(names, <span class="st">&quot;.gz&quot;</span>, <span class="dt">sep =</span> <span class="st">&quot;&quot;</span>)
    <span class="kw">print</span>(names)
    <span class="kw">print</span>(fnames)
    
    <span class="co"># loop over chunks of size &#39;chunk_size&#39;</span>
    for(i in <span class="dv">1</span>:n_chunks) {
        name1 &lt;-<span class="st"> </span>names[i]
        fname1 &lt;-<span class="st"> </span>fnames[i]
        idx &lt;-<span class="st"> </span>n1[i]:n2[i]
        
        <span class="kw">cat</span>(<span class="st">&quot;   &quot;</span>, name1, <span class="kw">length</span>(idx), idx[<span class="dv">1</span>], idx[<span class="kw">length</span>(idx)], <span class="st">&quot;</span><span class="ch">\n</span><span class="st">&quot;</span>)

        <span class="co"># find sentences</span>
        <span class="kw">assign</span>( name1, <span class="kw">sapply</span>(data.what[idx], <span class="dt">FUN =</span> find_sentences, <span class="dt">USE.NAMES =</span> <span class="ot">FALSE</span>) %&gt;%<span class="st"> </span>unlist )
        con &lt;-<span class="st"> </span><span class="kw">gzfile</span>(fname1, <span class="dt">open =</span> <span class="st">&quot;w&quot;</span>)

        <span class="co"># write sentences for this chunk to file</span>
        <span class="kw">writeLines</span>(<span class="kw">get</span>(name1), <span class="dt">con =</span> con)
        <span class="kw">close</span>(con)
    }
    <span class="kw">rm</span>(i, n1, n2, n_chunks)
}</code></pre>
<p>The stats table, with the added <em>number of sentences</em> is now as follows:</p>
<table>
<thead>
<tr class="header">
<th align="left"></th>
<th align="right">blogs</th>
<th align="right">news</th>
<th align="right">twitter</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">lines</td>
<td align="right">793096</td>
<td align="right">957228</td>
<td align="right">2072644</td>
</tr>
<tr class="even">
<td align="left">words</td>
<td align="right">36606082</td>
<td align="right">33908035</td>
<td align="right">29072455</td>
</tr>
<tr class="odd">
<td align="left">characters</td>
<td align="right">207683984</td>
<td align="right">206471971</td>
<td align="right">162152503</td>
</tr>
<tr class="even">
<td align="left">sentences</td>
<td align="right">2136323</td>
<td align="right">1819982</td>
<td align="right">3257495</td>
</tr>
<tr class="odd">
<td align="left">sentences_per_line</td>
<td align="right">2.694</td>
<td align="right">1.901</td>
<td align="right">1.572</td>
</tr>
</tbody>
</table>
<p>The list of sentences by dataset were then merged into a single master list.</p>
<div id="some-further-cleaning-of-text-perl" class="section level3">
<h3>Some further cleaning of text (<em>perl</em>)</h3>
<p>After the tokenization into sentences, some more fixes were applied with <em>perl scripts</em> (sources posted on <a href="https://github.com/pedrosan/DataScienceExamples/tree/master/myTextPredictr/MilestoneReport/scripts">GitHub</a></p>
<pre class="sourceCode bash"><code class="sourceCode bash"><span class="kw">gzip</span> -dc all.sentences.ALL.gz <span class="kw">|</span> <span class="kw">\</span>
    <span class="kw">./sentences_cleaning-v1.pl</span> <span class="kw">|</span> <span class="kw">\</span>
    <span class="kw">awk</span> <span class="st">&#39;{if(NF &gt; 1){print $0}}&#39;</span> <span class="kw">|</span> <span class="kw">\</span>
    <span class="kw">./sentences_cleaning-v2.pl</span> <span class="kw">|</span> <span class="kw">\</span>
    <span class="kw">./sentences_cleaning-v2.pl</span> <span class="kw">|</span> <span class="kw">\</span>
    <span class="kw">./sentences_cleaning-v2.pl</span> <span class="kw">|</span> <span class="kw">\</span>
    <span class="kw">./sentences_cleaning-v3.pl</span> <span class="kw">|</span> <span class="kw">\</span>
    <span class="kw">./sentences_cleaning-v2.pl</span></code></pre>
<p>The repetition of script #2 turned out to be an easier approach instead of writing more complex regular expressions.</p>
<p>The processed sentences were then filtered based on</p>
<ul>
<li>Number of words <span class="math">\(\ge 3\)</span> (<code>NF</code> in <em>awk</em>).
<ul>
<li>This criterion is simply due to the fact that we want sentences long enough to yield 3-grams.</li>
</ul></li>
<li>A <em>maximum</em> value of a ratio variable defined as <span class="math">\(\frac{(length - NF + 1)}{NF} \le 7.0\)</span>
<ul>
<li>This ratio cuts out particularly pathological sentences, including non-sense ‚Äúwords‚Äù (typically from twitter text).</li>
</ul></li>
</ul>
<pre class="sourceCode bash"><code class="sourceCode bash"><span class="kw">awk</span> <span class="st">&#39;{if(NF &lt; 3){next}; ratio = (length - NF + 1)/NF; if(ratio &gt; 7.0){next}; print $0}&#39;</span> </code></pre>
</div>
<div id="removing-stop-words-r" class="section level3">
<h3>Removing <em>stop words</em> (R)</h3>
<p>After more thinking I decided to partially reconsider my decision against removal of <em>stop words</em> and remove a controlled, selected list of them.</p>
<pre class="sourceCode r"><code class="sourceCode r">my_stop_words &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;a&quot;</span>, <span class="st">&quot;an&quot;</span>, <span class="st">&quot;as&quot;</span>, <span class="st">&quot;at&quot;</span>, <span class="st">&quot;no&quot;</span>, <span class="st">&quot;of&quot;</span>, <span class="st">&quot;on&quot;</span>, <span class="st">&quot;or&quot;</span>, 
                   <span class="st">&quot;by&quot;</span>, <span class="st">&quot;so&quot;</span>, <span class="st">&quot;up&quot;</span>, <span class="st">&quot;or&quot;</span>, <span class="st">&quot;no&quot;</span>, <span class="st">&quot;in&quot;</span>, <span class="st">&quot;to&quot;</span>, <span class="st">&quot;rt&quot;</span>)

<span class="co"># fixing extra spaces left by removing stop words</span>
all.sentences &lt;-<span class="st"> </span><span class="kw">removeWords</span>(all.sentences.ALL, my_stop_words) %&gt;%<span class="st"> </span>
<span class="st">                        </span><span class="kw">gsub</span>(<span class="st">&quot; +&quot;</span>, <span class="st">&quot; &quot;</span>, . , <span class="dt">perl =</span> <span class="ot">TRUE</span>) %&gt;%<span class="st"> </span>
<span class="st">                        </span><span class="kw">gsub</span>(<span class="st">&quot;^ +&quot;</span>, <span class="st">&quot;&quot;</span>, . , <span class="dt">perl =</span> <span class="ot">TRUE</span>) %&gt;%<span class="st"> </span>
<span class="st">                        </span><span class="kw">gsub</span>(<span class="st">&quot; +$&quot;</span>, <span class="st">&quot;&quot;</span>, . , <span class="dt">perl =</span> <span class="ot">TRUE</span>)</code></pre>
<p><a name="ANALYSIS-STEP2"></a></p>
</div>
</div>
<div id="step-2-n-grams-tokenization" class="section level2">
<h2>Step 2 : <em>n-grams</em> Tokenization</h2>
<p><a href="#CONTENT">Back to TOC</a></p>
<p>For the <em>n-grams</em> tokenization I used the <strong>RWeka Tokenizer</strong> <code>NGramTokenizer</code>, passing to it a list of <em>token delimiters</em>.</p>
<p>I have not been able to run <code>NGramTokenizer()</code> on the full vector of sentences for each data set. It fails on some variation of memory-allocation related error (that honestly does not make much sense to me considering that I am running it on machines with 12GB of RAM).</p>
<p>So, I am processing data in chunks of 25,000 sentences, as <em>exemplified</em> by this block of code (the <em>n-grams</em> data for the following section are loaded from saved previous analysis).</p>
<p>I extracted <em>n-grams</em> for <span class="math">\(n = 3, 4, 5\)</span>, with the code shown below:</p>
<button class="toggle_code">
<strong>show n-grams tokenization code</strong>
</button>
<pre class="sourceCode r"><code class="sourceCode r">token_delim &lt;-<span class="st"> &quot; </span><span class="ch">\\</span><span class="st">r</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">t.,;:</span><span class="ch">\&quot;</span><span class="st">()?!&quot;</span>
nl.chunk &lt;-<span class="st"> </span><span class="dv">25000</span>

<span class="kw">gc</span>()
<span class="kw">cat</span>(<span class="st">&quot; *** Tokenizing n-grams in WHOLE dataset [&quot;</span>, <span class="kw">my_date</span>(), <span class="st">&quot;]----------------------------------------</span><span class="ch">\n</span><span class="st">&quot;</span>)

len.all.sentences &lt;-<span class="st"> </span><span class="kw">length</span>(all.sentences)
<span class="kw">cat</span>(<span class="st">&quot; *** Number of sentences in the WHOLE data set : &quot;</span>, len.all.sentences, <span class="st">&quot;</span><span class="ch">\n</span><span class="st">&quot;</span>)

<span class="co"># define variable used to filter sentences long enough for n-grams of length N</span>
subs &lt;-<span class="st"> </span><span class="kw">strsplit</span>(all.sentences, <span class="dt">split =</span> <span class="st">&quot;[ ;,.</span><span class="ch">\&quot;\t\r\n</span><span class="st">()!?]+&quot;</span>)
nstr.subs  &lt;-<span class="st"> </span><span class="kw">sapply</span>(subs, <span class="dt">FUN =</span> function(x) { <span class="kw">length</span>(<span class="kw">unlist</span>(x)) }, <span class="dt">USE.NAMES =</span> <span class="ot">FALSE</span>)
<span class="kw">rm</span>(subs)

for( ngram_size in <span class="dv">3</span>:<span class="dv">5</span> ) {
    <span class="kw">cat</span>(<span class="st">&quot; *** Tokenizing : WHOLE : &quot;</span>, ngram_size, <span class="st">&quot;-grams ----------------------------------------</span><span class="ch">\n</span><span class="st">&quot;</span>)
    
    good.sentences &lt;-<span class="st"> </span>all.sentences[nstr.subs &gt;=<span class="st"> </span>ngram_size]
    len.good &lt;-<span class="st"> </span><span class="kw">length</span>(good.sentences)
    <span class="kw">cat</span>(<span class="st">&quot;   Sentences with good length ( &gt;=&quot;</span>, ngram_size, <span class="st">&quot;) : &quot;</span>, <span class="kw">sprintf</span>(<span class="st">&quot;%7d&quot;</span>, len.good), <span class="st">&quot;</span><span class="ch">\n</span><span class="st">&quot;</span>)
    <span class="kw">cat</span>(<span class="st">&quot;   Sentences with good length ( &gt;=&quot;</span>, ngram_size, <span class="st">&quot;) : &quot;</span>, <span class="kw">sprintf</span>(<span class="st">&quot;%7d&quot;</span>, len.good), 
           <span class="st">&quot;(of &quot;</span>, <span class="kw">sprintf</span>(<span class="st">&quot;%7d&quot;</span>, len.all.sentences), <span class="st">&quot;)</span><span class="ch">\n</span><span class="st">&quot;</span>)

    n_chunks &lt;-<span class="st"> </span><span class="kw">floor</span>(len.good/nl.chunk) +<span class="st"> </span><span class="dv">1</span>
    n1 &lt;-<span class="st"> </span>((<span class="dv">1</span>:n_chunks)-<span class="dv">1</span>)*nl.chunk +<span class="st"> </span><span class="dv">1</span>
    n2 &lt;-<span class="st"> </span>(<span class="dv">1</span>:n_chunks)*nl.chunk
    n2[n_chunks] &lt;-<span class="st"> </span>len.good

    names &lt;-<span class="st"> </span><span class="kw">paste</span>(<span class="st">&quot;n&quot;</span>, <span class="kw">sprintf</span>(<span class="st">&quot;%1d&quot;</span>, ngram_size), <span class="st">&quot;grams.blogs.&quot;</span>, <span class="kw">sprintf</span>(<span class="st">&quot;%03d&quot;</span>, (<span class="dv">1</span>:n_chunks)), <span class="dt">sep =</span> <span class="st">&quot;&quot;</span>)
    fnames &lt;-<span class="st"> </span><span class="kw">paste</span>(<span class="st">&quot;output/&quot;</span>, names, <span class="st">&quot;.gz&quot;</span>, <span class="dt">sep =</span> <span class="st">&quot;&quot;</span>)
    
    for(i in <span class="dv">1</span>:n_chunks) {
        name1 &lt;-<span class="st"> </span>names[i]
        fname1 &lt;-<span class="st"> </span>fnames[i]
        idx &lt;-<span class="st"> </span>n1[i]:n2[i]
    
        <span class="kw">cat</span>(<span class="st">&quot;  [&quot;</span>, <span class="kw">sprintf</span>(<span class="st">&quot;%3d&quot;</span>, i), <span class="st">&quot;/&quot;</span>, <span class="kw">sprintf</span>(<span class="st">&quot;%3d&quot;</span>, n_chunks), <span class="st">&quot;]  &quot;</span>, 
               name1, <span class="kw">length</span>(idx), idx[<span class="dv">1</span>], idx[<span class="kw">length</span>(idx)], <span class="st">&quot;</span><span class="ch">\n</span><span class="st">&quot;</span>)
    
        <span class="co"># tokenize to n-grams</span>
        <span class="kw">assign</span>( name1, <span class="kw">NGramTokenizer</span>(good.sentences[idx], 
                <span class="kw">Weka_control</span>(<span class="dt">min =</span> ngram_size, <span class="dt">max =</span> ngram_size, <span class="dt">delimiters =</span> token_delim)) )
    
        <span class="co"># write to file n-grams from this chunk</span>
        con &lt;-<span class="st"> </span><span class="kw">gzfile</span>(fname1, <span class="dt">open =</span> <span class="st">&quot;w&quot;</span>)
        <span class="kw">writeLines</span>(<span class="kw">get</span>(name1), <span class="dt">con =</span> con)
        <span class="kw">close</span>(con)

        <span class="kw">gc</span>()
    }

    <span class="co"># Combining chunks into one n-gram vector</span>
    size.ngrams &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, n_chunks)
    total_length &lt;-<span class="st"> </span><span class="dv">0</span> 
    for(i in <span class="dv">1</span>:n_chunks) {
        name1 &lt;-<span class="st"> </span>names[i]
        this_length &lt;-<span class="st"> </span><span class="kw">length</span>(<span class="kw">get</span>(name1))
        size.ngrams[i] &lt;-<span class="st"> </span>this_length
        total_length &lt;-<span class="st"> </span>total_length +<span class="st"> </span>this_length
        <span class="kw">cat</span>(<span class="st">&quot;  [&quot;</span>, <span class="kw">sprintf</span>(<span class="st">&quot;%3d&quot;</span>, i), <span class="st">&quot;/&quot;</span>, <span class="kw">sprintf</span>(<span class="st">&quot;%3d&quot;</span>, n_chunks), 
               <span class="st">&quot;]  length of &quot;</span>, name1, <span class="st">&quot; = &quot;</span>, this_length, <span class="st">&quot;</span><span class="ch">\n</span><span class="st">&quot;</span>)
    }
    <span class="kw">cat</span>(<span class="st">&quot;    Total Length = &quot;</span>, total_length, <span class="st">&quot;</span><span class="ch">\n</span><span class="st">&quot;</span>)
    
    name_for_all_ngrams &lt;-<span class="st"> </span><span class="kw">paste</span>(<span class="st">&quot;n&quot;</span>, <span class="kw">sprintf</span>(<span class="st">&quot;%1d&quot;</span>, ngram_size), <span class="st">&quot;grams.blogs.all&quot;</span>, <span class="dt">sep =</span> <span class="st">&quot;&quot;</span>)
    temp_all_ngrams &lt;-<span class="st"> </span><span class="kw">vector</span>(<span class="dt">mode =</span> <span class="st">&quot;character&quot;</span>, <span class="dt">length =</span> total_length)
    ivec &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">0</span>, <span class="kw">cumsum</span>(size.ngrams))
    for(i in <span class="dv">1</span>:n_chunks) {
        i1 &lt;-<span class="st"> </span>ivec[i] +<span class="st"> </span><span class="dv">1</span>
        i2 &lt;-<span class="st"> </span>ivec[i<span class="dv">+1</span>]
        name &lt;-<span class="st"> </span>names[i]
        <span class="kw">cat</span>(<span class="st">&quot;   &quot;</span>, i, i1, i2, name, <span class="st">&quot;</span><span class="ch">\n</span><span class="st">&quot;</span>)
        temp_all_ngrams[i1:i2] &lt;-<span class="st"> </span><span class="kw">get</span>(name)
    }

    <span class="kw">assign</span>( name_for_all_ngrams, temp_all_ngrams )

    <span class="co"># write to file all n-grams</span>
    fname &lt;-<span class="st"> </span><span class="kw">paste</span>(<span class="st">&quot;output/&quot;</span>, <span class="st">&quot;n&quot;</span>, <span class="kw">sprintf</span>(<span class="st">&quot;%1d&quot;</span>, ngram_size), <span class="st">&quot;grams.blogs.all.gz&quot;</span>, <span class="dt">sep =</span> <span class="st">&quot;&quot;</span>)
    con &lt;-<span class="st"> </span><span class="kw">gzfile</span>(fname, <span class="dt">open =</span> <span class="st">&quot;w&quot;</span>)
    <span class="kw">writeLines</span>(temp_all_ngrams, <span class="dt">con =</span> con)
    <span class="kw">close</span>(con)

    <span class="co"># cleaning</span>
    <span class="kw">rm</span>(good.sentences, len.good, temp_all_ngrams)
    
    <span class="kw">rm</span>(i, n1, n2, n_chunks)
    <span class="kw">ls</span>(<span class="dt">pattern =</span> <span class="st">&quot;^n[1-6]grams.blogs.[0-9]&quot;</span>)
    <span class="kw">rm</span>(<span class="dt">list =</span> <span class="kw">ls</span>(<span class="dt">pattern =</span> <span class="st">&quot;^n[1-6]grams.blogs.[0-9]&quot;</span>) )
    <span class="kw">gc</span>()
}</code></pre>
<p><a name="ngrams-cleaning_and_reclassification"></a></p>
<div id="cleaning-and-re-classification-of-n-grams" class="section level3">
<h3>Cleaning and <em>re-classification</em> of n-grams</h3>
<p>A small fraction of the n-grams so produced contains no-ASCII characters, and it makes sense to simply drop these n-grams.<br />For instance with:</p>
<pre class="sourceCode bash"><code class="sourceCode bash"><span class="kw">gzip</span> -dc n5grams.all.gz <span class="kw">|</span> <span class="kw">grep</span> -P -v <span class="st">&#39;([^\x00-\x7F]+)&#39;</span> </code></pre>
<p>Another <em>perl script</em> (<code>ngrams-reprocess_clean_and_classify.pl</code>, find it on <a href="https://github.com/pedrosan/DataScienceExamples/tree/master/myTextPredictr/MilestoneReport/scripts">GitHub</a>) then</p>
<ul>
<li>Cleans up some problematic words or rejects some pathologically bad n-grams.</li>
<li>Reclassifies n-grams based on their true adjusted number of words.</li>
</ul>
<pre class="sourceCode bash"><code class="sourceCode bash"><span class="kw">gzip</span> -dc n5grams.filtered_for_nonASCII.gz <span class="kw">|</span> <span class="kw">./scripts/ngrams-reprocess_clean_and_classify.pl</span> -go -print</code></pre>
<p>This script produces a set of 7 files (<code>tmp_n*</code>) containing: * good 1-grams * good 2-grams * good 3-grams * good 4-grams * good 5-grams * n &gt; 5 grams * Trashed n-grams (full of problem not worth dealing with)</p>
<p>Finally, we merge the n-grams of each order produced by the above script reprocessing all original n-grams.</p>
<p><a name="ngrams_tables"></a></p>
</div>
<div id="a-look-at-the-n-grams" class="section level3">
<h3>A look at the n-grams</h3>
<p><a href="#CONTENT">Back to TOC</a></p>
<p><strong>NOTE:</strong> the following results refer to the analysis of a subset of sentences (20%).</p>
<p>From the <em>n-grams</em> vectors we can compute frequencies, which will be an important basis for the prediction algorithms.</p>
<p>For now we can take a peek at what are the most frequent <em>3-grams</em> and <em>4-grams</em> in the three datasets.</p>
<button class="toggle_code">
show code for blogs
</button>
<pre class="sourceCode r"><code class="sourceCode r">n3g.blogs.freq &lt;-<span class="st"> </span><span class="kw">as.data.frame</span>(<span class="kw">table</span>(n3grams.blogs.all), <span class="dt">stringsAsFactors =</span> <span class="ot">FALSE</span>)
n3g.blogs.freq &lt;-<span class="st"> </span>n3g.blogs.freq[<span class="kw">order</span>(n3g.blogs.freq$Freq, <span class="dt">decreasing =</span> <span class="ot">TRUE</span>), ]
<span class="kw">row.names</span>(n3g.blogs.freq) &lt;-<span class="st"> </span><span class="ot">NULL</span>

n4g.blogs.freq &lt;-<span class="st"> </span><span class="kw">as.data.frame</span>(<span class="kw">table</span>(n4grams.blogs.all), <span class="dt">stringsAsFactors =</span> <span class="ot">FALSE</span>)
n4g.blogs.freq &lt;-<span class="st"> </span>n4g.blogs.freq[<span class="kw">order</span>(n4g.blogs.freq$Freq, <span class="dt">decreasing =</span> <span class="ot">TRUE</span>), ]
<span class="kw">row.names</span>(n4g.blogs.freq) &lt;-<span class="st"> </span><span class="ot">NULL</span>

<span class="kw">colnames</span>(n3g.blogs.freq) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;ngram&quot;</span>, <span class="st">&quot;count&quot;</span>)
<span class="kw">colnames</span>(n4g.blogs.freq) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;ngram&quot;</span>, <span class="st">&quot;count&quot;</span>)</code></pre>
<br />
<button class="toggle_code">
show code for news
</button>
<pre class="sourceCode r"><code class="sourceCode r">n3g.news.freq &lt;-<span class="st"> </span><span class="kw">as.data.frame</span>(<span class="kw">table</span>(n3grams.news.all), <span class="dt">stringsAsFactors =</span> <span class="ot">FALSE</span>)
n3g.news.freq &lt;-<span class="st"> </span>n3g.news.freq[<span class="kw">order</span>(n3g.news.freq$Freq, <span class="dt">decreasing =</span> <span class="ot">TRUE</span>), ]
<span class="kw">row.names</span>(n3g.news.freq) &lt;-<span class="st"> </span><span class="ot">NULL</span>

n4g.news.freq &lt;-<span class="st"> </span><span class="kw">as.data.frame</span>(<span class="kw">table</span>(n4grams.news.all), <span class="dt">stringsAsFactors =</span> <span class="ot">FALSE</span>)
n4g.news.freq &lt;-<span class="st"> </span>n4g.news.freq[<span class="kw">order</span>(n4g.news.freq$Freq, <span class="dt">decreasing =</span> <span class="ot">TRUE</span>), ]
<span class="kw">row.names</span>(n4g.news.freq) &lt;-<span class="st"> </span><span class="ot">NULL</span>

<span class="kw">colnames</span>(n3g.news.freq) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;ngram&quot;</span>, <span class="st">&quot;count&quot;</span>)
<span class="kw">colnames</span>(n4g.news.freq) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;ngram&quot;</span>, <span class="st">&quot;count&quot;</span>)</code></pre>
<br />
<button class="toggle_code">
show code for twitter
</button>
<pre class="sourceCode r"><code class="sourceCode r">n3g.twitter.freq &lt;-<span class="st"> </span><span class="kw">as.data.frame</span>(<span class="kw">table</span>(n3grams.twitter.all), <span class="dt">stringsAsFactors =</span> <span class="ot">FALSE</span>)
n3g.twitter.freq &lt;-<span class="st"> </span>n3g.twitter.freq[<span class="kw">order</span>(n3g.twitter.freq$Freq, <span class="dt">decreasing =</span> <span class="ot">TRUE</span>), ]
<span class="kw">row.names</span>(n3g.twitter.freq) &lt;-<span class="st"> </span><span class="ot">NULL</span>

n4g.twitter.freq &lt;-<span class="st"> </span><span class="kw">as.data.frame</span>(<span class="kw">table</span>(n4grams.twitter.all), <span class="dt">stringsAsFactors =</span> <span class="ot">FALSE</span>)
n4g.twitter.freq &lt;-<span class="st"> </span>n4g.twitter.freq[<span class="kw">order</span>(n4g.twitter.freq$Freq, <span class="dt">decreasing =</span> <span class="ot">TRUE</span>), ]
<span class="kw">row.names</span>(n4g.twitter.freq) &lt;-<span class="st"> </span><span class="ot">NULL</span>

<span class="kw">colnames</span>(n3g.twitter.freq) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;ngram&quot;</span>, <span class="st">&quot;count&quot;</span>)
<span class="kw">colnames</span>(n4g.twitter.freq) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;ngram&quot;</span>, <span class="st">&quot;count&quot;</span>)</code></pre>
<div id="grams" class="section level4">
<h4><strong>3-grams</strong></h4>
<button class="toggle_code">
show code
</button>
<pre class="sourceCode r"><code class="sourceCode r">tmp3.df &lt;-<span class="st"> </span><span class="kw">cbind</span>(<span class="kw">head</span>(n3g.blogs.top500, <span class="dv">20</span>), 
                 <span class="kw">head</span>(n3g.news.top500, <span class="dv">20</span>), 
                 <span class="kw">head</span>(n3g.twitter.top500, <span class="dv">20</span>)) 
tmp4.df &lt;-<span class="st"> </span><span class="kw">cbind</span>(<span class="kw">head</span>(n4g.blogs.top500, <span class="dv">20</span>), 
                 <span class="kw">head</span>(n4g.news.top500, <span class="dv">20</span>), 
                 <span class="kw">head</span>(n4g.twitter.top500, <span class="dv">20</span>)) 
<span class="kw">colnames</span>(tmp4.df) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;ngram_blogs&quot;</span>, <span class="st">&quot;count&quot;</span>, <span class="st">&quot;ngram_news&quot;</span>, <span class="st">&quot;count&quot;</span>, <span class="st">&quot;ngram_twitter&quot;</span>, <span class="st">&quot;count&quot;</span>)
<span class="kw">colnames</span>(tmp4.df) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;ngram_blogs&quot;</span>, <span class="st">&quot;count&quot;</span>, <span class="st">&quot;ngram_news&quot;</span>, <span class="st">&quot;count&quot;</span>, <span class="st">&quot;ngram_twitter&quot;</span>, <span class="st">&quot;count&quot;</span>)
<span class="co"># saveRDS(tmp3.df, file = &quot;tmp_n3g_table.RDS&quot;)</span></code></pre>
<ul>
<li><strong>blogs</strong></li>
</ul>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">print</span>(tmp3.df[, <span class="dv">1</span>:<span class="dv">2</span>], <span class="dt">print.gap =</span> <span class="dv">3</span>, <span class="dt">right =</span> <span class="ot">FALSE</span>)
<span class="co">#      ngram           count</span>
<span class="co"># 1    one of the      4416 </span>
<span class="co"># 2    a lot of        3613 </span>
<span class="co"># 3    to be a         2078 </span>
<span class="co"># 4    it was a        2076 </span>
<span class="co"># 5    as well as      2067 </span>
<span class="co"># 6    some of the     1988 </span>
<span class="co"># 7    the end of      1974 </span>
<span class="co"># 8    out of the      1954 </span>
<span class="co"># 9    be able to      1927 </span>
<span class="co"># 10   i want to       1882 </span>
<span class="co"># 11   a couple of     1828 </span>
<span class="co"># 12   the fact that   1596 </span>
<span class="co"># 13   this is a       1592 </span>
<span class="co"># 14   the rest of     1539 </span>
<span class="co"># 15   going to be     1521 </span>
<span class="co"># 16   part of the     1478 </span>
<span class="co"># 17   i_am going to   1448 </span>
<span class="co"># 18   i do_not know   1425 </span>
<span class="co"># 19   one of my       1408 </span>
<span class="co"># 20   i had to        1373</span></code></pre>
<ul>
<li><strong>news</strong></li>
</ul>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">print</span>(tmp3.df[, <span class="dv">3</span>:<span class="dv">4</span>], <span class="dt">print.gap =</span> <span class="dv">3</span>, <span class="dt">right =</span> <span class="ot">FALSE</span>)
<span class="co">#      ngram                             count</span>
<span class="co"># 1    the united states                 1324 </span>
<span class="co"># 2    the first time                    1249 </span>
<span class="co"># 3    for the first                     1021 </span>
<span class="co"># 4    more than &lt;DOLLARAMOUNT&gt;          1000 </span>
<span class="co"># 5    the end the                        896 </span>
<span class="co"># 6    it would be                        751 </span>
<span class="co"># 7    it was the                         722 </span>
<span class="co"># 8    the fact that                      690 </span>
<span class="co"># 9    &lt;DOLLARAMOUNT&gt; - &lt;DOLLARAMOUNT&gt;    680 </span>
<span class="co"># 10   this is the                        679 </span>
<span class="co"># 11   the rest the                       676 </span>
<span class="co"># 12   said he was                        667 </span>
<span class="co"># 13   he said he                         655 </span>
<span class="co"># 14   i do_not think                     652 </span>
<span class="co"># 15   the new york                       651 </span>
<span class="co"># 16   he said the                        628 </span>
<span class="co"># 17   i do_not know                      626 </span>
<span class="co"># 18   for more than                      622 </span>
<span class="co"># 19   the same time                      578 </span>
<span class="co"># 20   when he was                        565</span></code></pre>
<ul>
<li><strong>twitter</strong></li>
</ul>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">print</span>(tmp3.df[, <span class="dv">5</span>:<span class="dv">6</span>], <span class="dt">print.gap =</span> <span class="dv">3</span>, <span class="dt">right =</span> <span class="ot">FALSE</span>)
<span class="co">#      ngram                           count</span>
<span class="co"># 1    thanks for the                  7135 </span>
<span class="co"># 2    thank you for                   2590 </span>
<span class="co"># 3    i love you                      2474 </span>
<span class="co"># 4    for the follow                  2334 </span>
<span class="co"># 5    for the rt                      1311 </span>
<span class="co"># 6    let me know                     1301 </span>
<span class="co"># 7    i do_not know                   1265 </span>
<span class="co"># 8    i feel like                     1179 </span>
<span class="co"># 9    i wish i                        1154 </span>
<span class="co"># 10   thanks for following            1048 </span>
<span class="co"># 11   you for the                     1013 </span>
<span class="co"># 12   i can_not wait                   968 </span>
<span class="co"># 13   &lt;HASHTAG&gt; &lt;HASHTAG&gt; &lt;HASHTAG&gt;    963 </span>
<span class="co"># 14   how are you                      960 </span>
<span class="co"># 15   for the &lt;HASHTAG&gt;                958 </span>
<span class="co"># 16   can_not wait for                 919 </span>
<span class="co"># 17   rt : i                           915 </span>
<span class="co"># 18   i think i                        895 </span>
<span class="co"># 19   if you want                      867 </span>
<span class="co"># 20   what do you                      858</span></code></pre>
</div>
<div id="grams-1" class="section level4">
<h4><strong>4-grams</strong></h4>
<ul>
<li><strong>blogs</strong></li>
</ul>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">print</span>(tmp4.df[, <span class="dv">1</span>:<span class="dv">2</span>], <span class="dt">print.gap =</span> <span class="dv">3</span>, <span class="dt">right =</span> <span class="ot">FALSE</span>)
<span class="co">#      ngram_blogs          count</span>
<span class="co"># 1    the end of the       1011 </span>
<span class="co"># 2    the rest of the       913 </span>
<span class="co"># 3    at the end of         872 </span>
<span class="co"># 4    at the same time      700 </span>
<span class="co"># 5    when it comes to      611 </span>
<span class="co"># 6    one of the most       610 </span>
<span class="co"># 7    to be able to         578 </span>
<span class="co"># 8    for the first time    565 </span>
<span class="co"># 9    in the middle of      519 </span>
<span class="co"># 10   if you want to        469 </span>
<span class="co"># 11   is one of the         462 </span>
<span class="co"># 12   i do_not want to      461 </span>
<span class="co"># 13   a bit of a            403 </span>
<span class="co"># 14   i was going to        395 </span>
<span class="co"># 15   on the other hand     393 </span>
<span class="co"># 16   i would like to       375 </span>
<span class="co"># 17   one of my favorite    350 </span>
<span class="co"># 18   as well as the        325 </span>
<span class="co"># 19   i was able to         304 </span>
<span class="co"># 20   is going to be        302</span></code></pre>
<ul>
<li><strong>news</strong></li>
</ul>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">print</span>(tmp4.df[, <span class="dv">3</span>:<span class="dv">4</span>], <span class="dt">print.gap =</span> <span class="dv">3</span>, <span class="dt">right =</span> <span class="ot">FALSE</span>)
<span class="co">#      ngram_news                                      count</span>
<span class="co"># 1    for the first time                              791  </span>
<span class="co"># 2    more than &lt;DOLLARAMOUNT&gt; million                398  </span>
<span class="co"># 3    the first time since                            195  </span>
<span class="co"># 4    more than &lt;DOLLARAMOUNT&gt; billion                150  </span>
<span class="co"># 5    for more than years                             138  </span>
<span class="co"># 6    feet &lt;DATE&gt; for &lt;DOLLARAMOUNT&gt;                  137  </span>
<span class="co"># 7    square feet &lt;DATE&gt; for                          137  </span>
<span class="co"># 8    &lt;DOLLARAMOUNT&gt; million &lt;DOLLARAMOUNT&gt; million   136  </span>
<span class="co"># 9    for the most part                               133  </span>
<span class="co"># 10   the past two years                              132  </span>
<span class="co"># 11   told the associated press                       132  </span>
<span class="co"># 12   the united states and                           131  </span>
<span class="co"># 13   i do_not know if                                126  </span>
<span class="co"># 14   the end the year                                126  </span>
<span class="co"># 15   the end the day                                 124  </span>
<span class="co"># 16   g fat g saturated                               118  </span>
<span class="co"># 17   the new york times                              118  </span>
<span class="co"># 18   dow jones industrial average                    114  </span>
<span class="co"># 19   be reached for comment                          112  </span>
<span class="co"># 20   i do_not know what                              112</span></code></pre>
<ul>
<li><strong>twitter</strong></li>
</ul>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">print</span>(tmp4.df[, <span class="dv">5</span>:<span class="dv">6</span>], <span class="dt">print.gap =</span> <span class="dv">3</span>, <span class="dt">right =</span> <span class="ot">FALSE</span>)
<span class="co">#      ngram_twitter                             count</span>
<span class="co"># 1    thanks for the follow                     1882 </span>
<span class="co"># 2    thanks for the rt                         1031 </span>
<span class="co"># 3    thank you for the                          916 </span>
<span class="co"># 4    for the first time                         513 </span>
<span class="co"># 5    i wish i could                             410 </span>
<span class="co"># 6    thanks for the &lt;HASHTAG&gt;                   375 </span>
<span class="co"># 7    rt : rt :                                  358 </span>
<span class="co"># 8    thanks for the mention                     358 </span>
<span class="co"># 9    let me know if                             330 </span>
<span class="co"># 10   &lt;HASHTAG&gt; &lt;HASHTAG&gt; &lt;HASHTAG&gt; &lt;HASHTAG&gt;    322 </span>
<span class="co"># 11   that awkward moment when                   299 </span>
<span class="co"># 12   what do you think                          292 </span>
<span class="co"># 13   thank you much for                         276 </span>
<span class="co"># 14   hope all is well                           266 </span>
<span class="co"># 15   can_not wait for the                       262 </span>
<span class="co"># 16   thanks for the shout                       257 </span>
<span class="co"># 17   for the shout out                          254 </span>
<span class="co"># 18   i thought it was                           243 </span>
<span class="co"># 19   thank you for following                    240 </span>
<span class="co"># 20   thank you for your                         232</span></code></pre>
<p>It is apparent that there some work will be necessary on the validation of the <em>n-grams</em>, or better still further text transformations, in particular of the <em>twitter</em> data set that ‚Äúsuffers‚Äù from the tendency of using <em>shorthand slang</em> (<em>e.g.</em> ‚Äúrt‚Äù for ‚Äúre-tweet‚Äù) that adds a lot of ‚Äúnoise‚Äù to the data.</p>
<p><a name="ngrams_plots"></a></p>
</div>
</div>
<div id="some-summary-plots-for-4-grams" class="section level3">
<h3>Some Summary Plots for 4-grams</h3>
<p><a href="#CONTENT">Back to TOC</a></p>
<p><a name="top30_mixed"></a></p>
<div id="top-30-all-mixed" class="section level4">
<h4>Top-30 all mixed</h4>
<button class="toggle_plot_code">
show plot code
</button>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># ECHO FALSE</span>
mycolors &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;deepskyblue3&quot;</span>, <span class="st">&quot;firebrick2&quot;</span>, <span class="st">&quot;forestgreen&quot;</span>)
data2pl &lt;-<span class="st"> </span>n4g.high.sorted[<span class="dv">1</span>:<span class="dv">30</span>, ]
bp_mix &lt;-<span class="st"> </span><span class="kw">ggplot</span>(data2pl, <span class="kw">aes</span>(<span class="dt">x =</span> <span class="kw">reorder</span>(ngram, count), <span class="dt">y =</span> count)) +<span class="st"> </span><span class="kw">theme_bw</span>() +<span class="st"> </span><span class="kw">coord_flip</span>() +<span class="st"> </span><span class="kw">xlab</span>(<span class="st">&quot;&quot;</span>) +<span class="st"> </span>
<span class="st">    </span><span class="kw">theme</span>(<span class="dt">plot.title =</span> <span class="kw">element_text</span>(<span class="dt">face =</span> <span class="st">&quot;bold&quot;</span>, <span class="dt">size =</span> <span class="dv">20</span>)) +<span class="st"> </span>
<span class="st">    </span><span class="kw">theme</span>(<span class="dt">axis.text =</span> <span class="kw">element_text</span>(<span class="dt">size =</span> <span class="dv">10</span>)) +<span class="st"> </span>
<span class="st">    </span><span class="kw">scale_fill_manual</span>(<span class="dt">values =</span> mycolors) +<span class="st"> </span>
<span class="st">    </span><span class="kw">ggtitle</span>(<span class="st">&quot;Top 30 4-grams : all sources&quot;</span>) +<span class="st"> </span>
<span class="st">    </span><span class="kw">geom_bar</span>(<span class="dt">stat =</span> <span class="st">&quot;identity&quot;</span>, <span class="kw">aes</span>(<span class="dt">fill =</span> flag)) +<span class="st"> </span>
<span class="st">    </span><span class="kw">geom_text</span>(<span class="kw">aes</span>(<span class="dt">label =</span> count, <span class="dt">hjust =</span> <span class="fl">1.1</span>, <span class="dt">size =</span> <span class="dv">10</span>), <span class="dt">col =</span> <span class="st">&quot;white&quot;</span>, <span class="dt">position =</span> <span class="kw">position_stack</span>())

bp_mix</code></pre>
<p><img src="figures/barplot_mix-1.png" title="" alt="" width="1000" style="display: block; margin: auto;" /></p>
<p><a name="top20_separate"></a></p>
</div>
<div id="top-20-by-data-source" class="section level4">
<h4>Top-20 by data source</h4>
<button class="toggle_plot_code">
show plot code
</button>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># ECHO FALSE</span>
data.blogs &lt;-<span class="st"> </span>n4g.blogs.top500[<span class="dv">1</span>:<span class="dv">20</span>, ]
bp_blogs &lt;-<span class="st"> </span><span class="kw">ggplot</span>(data.blogs, <span class="kw">aes</span>(<span class="dt">x =</span> <span class="kw">reorder</span>(ngram, count), <span class="dt">y =</span> count)) +<span class="st"> </span><span class="kw">theme_bw</span>() +<span class="st"> </span><span class="kw">coord_flip</span>() +<span class="st"> </span><span class="kw">xlab</span>(<span class="st">&quot;&quot;</span>) +<span class="st"> </span>
<span class="st">    </span><span class="kw">theme</span>(<span class="dt">plot.title =</span> <span class="kw">element_text</span>(<span class="dt">face =</span> <span class="st">&quot;bold&quot;</span>, <span class="dt">size =</span> <span class="dv">20</span>)) +<span class="st"> </span>
<span class="st">    </span><span class="kw">theme</span>(<span class="dt">axis.text =</span> <span class="kw">element_text</span>(<span class="dt">size =</span> <span class="dv">10</span>)) +<span class="st"> </span>
<span class="st">    </span><span class="kw">theme</span>(<span class="dt">legend.position =</span> <span class="st">&quot;none&quot;</span>) +<span class="st"> </span>
<span class="st">    </span><span class="kw">ggtitle</span>(<span class="st">&quot;Top 20 4-grams : blogs&quot;</span>) +<span class="st"> </span>
<span class="st">    </span><span class="kw">geom_bar</span>(<span class="dt">stat =</span> <span class="st">&quot;identity&quot;</span>, <span class="dt">fill =</span> mycolors[<span class="dv">1</span>]) +<span class="st"> </span>
<span class="st">    </span><span class="kw">geom_text</span>(<span class="kw">aes</span>(<span class="dt">label =</span> count, <span class="dt">y =</span> <span class="dv">100</span>, <span class="dt">size =</span> <span class="dv">10</span>), <span class="dt">col =</span> <span class="st">&quot;white&quot;</span>) 

data.news &lt;-<span class="st"> </span>n4g.news.top500[<span class="dv">1</span>:<span class="dv">20</span>, ]
bp_news &lt;-<span class="st"> </span><span class="kw">ggplot</span>(data.news, <span class="kw">aes</span>(<span class="dt">x =</span> <span class="kw">reorder</span>(ngram, count), <span class="dt">y =</span> count)) +<span class="st"> </span><span class="kw">theme_bw</span>() +<span class="st"> </span><span class="kw">coord_flip</span>() +<span class="st"> </span><span class="kw">xlab</span>(<span class="st">&quot;&quot;</span>) +<span class="st"> </span>
<span class="st">    </span><span class="kw">theme</span>(<span class="dt">plot.title =</span> <span class="kw">element_text</span>(<span class="dt">face =</span> <span class="st">&quot;bold&quot;</span>, <span class="dt">size =</span> <span class="dv">20</span>)) +<span class="st"> </span>
<span class="st">    </span><span class="kw">theme</span>(<span class="dt">axis.text =</span> <span class="kw">element_text</span>(<span class="dt">size =</span> <span class="dv">10</span>)) +<span class="st"> </span>
<span class="st">    </span><span class="kw">theme</span>(<span class="dt">legend.position =</span> <span class="st">&quot;none&quot;</span>) +<span class="st"> </span>
<span class="st">    </span><span class="kw">ggtitle</span>(<span class="st">&quot;Top 20 4-grams : news&quot;</span>) +<span class="st"> </span>
<span class="st">    </span><span class="kw">geom_bar</span>(<span class="dt">stat =</span> <span class="st">&quot;identity&quot;</span>, <span class="dt">fill =</span> mycolors[<span class="dv">2</span>]) +<span class="st"> </span>
<span class="st">    </span><span class="kw">geom_text</span>(<span class="kw">aes</span>(<span class="dt">label =</span> count, <span class="dt">y =</span> <span class="dv">100</span>, <span class="dt">size =</span> <span class="dv">10</span>), <span class="dt">col =</span> <span class="st">&quot;white&quot;</span>) 

data.twitter &lt;-<span class="st"> </span>n4g.twitter.top500[<span class="dv">1</span>:<span class="dv">20</span>, ]
bp_twitter &lt;-<span class="st"> </span><span class="kw">ggplot</span>(data.twitter, <span class="kw">aes</span>(<span class="dt">x =</span> <span class="kw">reorder</span>(ngram, count), <span class="dt">y =</span> count)) +<span class="st"> </span><span class="kw">theme_bw</span>() +<span class="st"> </span><span class="kw">coord_flip</span>() +<span class="st"> </span><span class="kw">xlab</span>(<span class="st">&quot;&quot;</span>) +<span class="st"> </span>
<span class="st">    </span><span class="kw">theme</span>(<span class="dt">plot.title =</span> <span class="kw">element_text</span>(<span class="dt">face =</span> <span class="st">&quot;bold&quot;</span>, <span class="dt">size =</span> <span class="dv">20</span>)) +<span class="st"> </span>
<span class="st">    </span><span class="kw">theme</span>(<span class="dt">axis.text =</span> <span class="kw">element_text</span>(<span class="dt">size =</span> <span class="dv">10</span>)) +<span class="st"> </span>
<span class="st">    </span><span class="kw">theme</span>(<span class="dt">legend.position =</span> <span class="st">&quot;none&quot;</span>) +<span class="st"> </span>
<span class="st">    </span><span class="kw">ggtitle</span>(<span class="st">&quot;Top 20 4-grams : twitter&quot;</span>) +<span class="st"> </span>
<span class="st">    </span><span class="kw">geom_bar</span>(<span class="dt">stat =</span> <span class="st">&quot;identity&quot;</span>, <span class="dt">fill =</span> mycolors[<span class="dv">3</span>]) +<span class="st"> </span>
<span class="st">    </span><span class="kw">geom_text</span>(<span class="kw">aes</span>(<span class="dt">label =</span> count, <span class="dt">y =</span> <span class="dv">100</span>, <span class="dt">size =</span> <span class="dv">10</span>), <span class="dt">col =</span> <span class="st">&quot;white&quot;</span>) 

<span class="kw">grid.arrange</span>(bp_blogs, bp_news, bp_twitter, <span class="dt">nrow =</span> <span class="dv">3</span>)</code></pre>
<p><img src="figures/barplot_together-1.png" title="" alt="" width="700" style="display: block; margin: auto;" /></p>
<hr class="thin_separator">
<p><a name="APPENDIX"></a></p>
</div>
</div>
</div>
</div>
<div id="appendix" class="section level1">
<h1>APPENDIX</h1>
<p><a href="#TOP">Back to the Top</a></p>
<p><a name="APPENDIX-my_functions"></a></p>
<div id="user-defined-functions" class="section level2">
<h2>User Defined Functions</h2>
<p>These are two handy functions used in the analysis.</p>
<ul>
<li>The first for reading the data.</li>
<li>The second is passed to <code>sapply()</code> to annotate sentences, allowing to work by row instead of converting the whole dataset into one document.</li>
</ul>
<button class="toggle_code">
show code
</button>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co">#===================================================================================================</span>
<span class="co"># modified readLines</span>

readByLine &lt;-<span class="st"> </span>function(fname, <span class="dt">check_nl =</span> <span class="ot">TRUE</span>, <span class="dt">skipNul =</span> <span class="ot">TRUE</span>) {
    if( check_nl ) {
        cmd.nl   &lt;-<span class="st"> </span><span class="kw">paste</span>(<span class="st">&quot;gzip -dc&quot;</span>, fname, <span class="st">&quot;| wc -l | awk &#39;{print $1}&#39;&quot;</span>, <span class="dt">sep =</span> <span class="st">&quot; &quot;</span>)
        nl   &lt;-<span class="st"> </span><span class="kw">system</span>(cmd.nl, <span class="dt">intern =</span> <span class="ot">TRUE</span>)
    } else {
        nl   &lt;-<span class="st"> </span>-1L
    }
    con &lt;-<span class="st"> </span><span class="kw">gzfile</span>(fname, <span class="dt">open =</span> <span class="st">&quot;r&quot;</span>)
    <span class="kw">on.exit</span>(<span class="kw">close</span>(con))
    <span class="kw">readLines</span>(con, <span class="dt">n =</span> nl, <span class="dt">skipNul =</span> skipNul) 
}

<span class="co">#===================================================================================================</span>
<span class="co"># to use w/ sapply for finer sentence splitting.</span>

find_sentences &lt;-<span class="st"> </span>function(x) {
    s &lt;-<span class="st"> </span><span class="kw">paste</span>(x, <span class="dt">collapse =</span> <span class="st">&quot; &quot;</span>) %&gt;%<span class="st"> </span><span class="kw">as.String</span>()
    a &lt;-<span class="st"> </span>NLP::<span class="kw">annotate</span>(s , sent_token_annotator) 
    <span class="kw">as.vector</span>(s[a])
}

<span class="co">#===================================================================================================</span></code></pre>
<p>More functions can be reviewed directly from the <a href="https://github.com/pedrosan/DataScienceExamples/tree/master/myTextPredictr/MilestoneReport/scripts">repository</a></p>
<p><a name="APPENDIX-regularize"></a></p>
</div>
<div id="summary-of-regularization-done-with-perl-scripts" class="section level2">
<h2>Summary of regularization done with <em>perl scripts</em></h2>
<p>Scripts source in <a href="https://github.com/pedrosan/DataScienceExamples/tree/master/myTextPredictr/MilestoneReport/scripts">this GitHub folder</a></p>
<div id="regularize_text-pass1.pl" class="section level4">
<h4><em>regularize_text-pass1.pl</em></h4>
<ul>
<li>CHARACTERS ‚ÄúHOMOGENEIZATION‚Äù
<ul>
<li>Normalize odd characters</li>
<li>Currency symbols</li>
<li>HTML tags</li>
<li>encoded apostrophe</li>
</ul></li>
<li>CHARACTER SEQUENCES
<ul>
<li>Cleaning of BEGIN / END of LINE</li>
<li>EOL ellipsis</li>
<li>EOL non-sense</li>
<li>word bracketed by *</li>
<li>substitution on ‚Äú|‚Äù that seem to be equivalent to end of sentences (i.e.¬†a period)</li>
<li>substitution of &lt;==/&lt;‚Äì and ==&gt;/‚Äì&gt; with ‚Äú;‚Äù</li>
<li>sequences of !, ?</li>
</ul></li>
<li>HASHTAGS
<ul>
<li>recognized (most) hashtags</li>
</ul></li>
</ul>
</div>
<div id="regularize_text-pass2.pl" class="section level4">
<h4><em>regularize_text-pass2.pl</em></h4>
<ul>
<li>NUMBER RELATED
<ul>
<li>dates</li>
<li>hours am/pm</li>
<li>hours a.m./p.m.</li>
<li>dollar amounts</li>
<li>percentages</li>
</ul></li>
<li>ACRONYMS
<ul>
<li>acronyms: US</li>
</ul></li>
<li>EMOTICONS
<ul>
<li>regular</li>
<li>(reverse) [not done]</li>
</ul></li>
<li>ELLIPSIS
<ul>
<li>INLINE ellipsis</li>
</ul></li>
<li>REPEATED CHARACTERS
<ul>
<li>DOLLAR</li>
<li>STAR</li>
<li>‚Äú+‚Äù</li>
<li>‚Äú-‚Äù</li>
<li>‚Äúa ‚Äì b‚Äù ==&gt; replace with ‚Äú,‚Äù</li>
<li>‚Äúa‚Äìb‚Äù ==&gt; replace with ‚Äú,‚Äù</li>
<li>‚Äú! ‚Äì A‚Äù ==&gt; REMOVE</li>
<li>‚ÄúA‚ÄìA[a ‚Äô]‚Äù ==&gt; replace with ‚Äú;‚Äù</li>
<li>‚Äúa‚Äì A‚Äù ==&gt; replace with ‚Äú;‚Äù</li>
<li>‚Äúa‚Äì a‚Äù ==&gt; replace with ‚Äú,‚Äù</li>
<li>LEAVE ONE (followed by space) : ,</li>
<li>LEAVE ONE : _ (the % is handled separately when dealing with percentages)</li>
<li>REMOVE ENTIRELY if 2+ : &lt; &gt; = #</li>
</ul></li>
</ul>
</div>
<div id="regularize_text-pass3.pl" class="section level4">
<h4><em>regularize_text-pass3.pl</em></h4>
<ul>
<li>CONTRACTIONS
<ul>
<li>‚Äôll ==&gt; _will / &quot; will&quot; ==&gt; _will</li>
<li>n‚Äôt ==&gt; _not</li>
<li>‚Äôre ==&gt; _are</li>
<li>‚Äôve ==&gt; _have</li>
<li>some additional ad hoc (e.g.¬†won‚Äôt ==&gt; will_not)</li>
<li>‚Äôs ==&gt; _s</li>
<li>additional possibly useful/meaningful replacements (e.g.¬†y‚Äôall)</li>
</ul></li>
<li>WHITE SPACES
<ul>
<li>squeezing extra white spaces</li>
<li>fixing some punctuation and white space</li>
</ul></li>
<li>PROFANITIES
<ul>
<li>catch the ‚Äú7 ones‚Äù and replace with tag <PROFANITY></li>
</ul></li>
</ul>
</div>
<div id="regularize_text-pass4.pl" class="section level4">
<h4><em>regularize_text-pass4.pl</em></h4>
<ul>
<li>ABBREVIATIONS: find and mark with tags standard/common abbreviations
<ul>
<li>month names</li>
<li>Mr, Mrs, Dr, ‚Ä¶</li>
</ul></li>
<li>MORE
<ul>
<li>Find and replace something like <NUMBER>-word</li>
<li>Remove genitives</li>
<li>Clean line endings preceded by spurious spaces</li>
<li>Replace: ‚Äô‚Äô ==&gt; &quot;</li>
</ul></li>
</ul>
</div>
<div id="regularize_text-pass5.pl" class="section level4">
<h4><em>regularize_text-pass5.pl</em></h4>
<ul>
<li>WEIRD CHARACTERS
<ul>
<li>Replace weird characters with <WEIRDO> tag</li>
</ul></li>
<li>TAGS CLEANING
<ul>
<li>Clean consecutive <TAGS></li>
<li>Remove <PERIOD></li>
<li>Remove <SPACE></li>
<li>Remove <WEIRDO></li>
<li>Remove quotes from single quoted words</li>
</ul></li>
<li>MORE
<ul>
<li>Clear row BEGINNINGS with non-alpha characters</li>
</ul></li>
</ul>
</div>
<div id="regularize_text-pass6.pl" class="section level4">
<h4><em>regularize_text-pass6.pl</em></h4>
<ul>
<li>MORE TAG-RELATED REGULARIZATIONS
<ul>
<li>FIX missed HASHTAGS at line beginning</li>
<li>FIX additional number capture</li>
<li>Emptying tags that were defined to capture the original expression (e.g. <EMOJ_*_EMOJ> ==&gt; <EMOTICON></li>
</ul></li>
</ul>
<p><a name="APPENDIX-post_sentence_cleaning"></a></p>
</div>
</div>
<div id="summary-of-post-sentence-tokenization-cleaning-with-perl-scripts" class="section level2">
<h2>Summary of post-sentence-tokenization cleaning (with <em>perl scripts</em>)</h2>
<p>Scripts source in <a href="https://github.com/pedrosan/DataScienceExamples/tree/master/myTextPredictr/MilestoneReport/scripts">this GitHub folder</a></p>
<div id="sentences-cleaning_1.pl" class="section level4">
<h4><em>sentences-cleaning_1.pl</em></h4>
<ul>
<li>CONTRACTIONS
<ul>
<li>‚Äôll ==&gt; _will / &quot; will&quot; ==&gt; _will</li>
<li>n‚Äôt ==&gt; _not</li>
<li>‚Äôre ==&gt; _are</li>
<li>‚Äôve ==&gt; _have</li>
<li>some additional ad hoc</li>
<li>‚Äôs ==&gt; _s</li>
<li>additional possibly useful/meaningful replacements (e.g.¬†y‚Äôall)</li>
</ul></li>
</ul>
</div>
<div id="sentences-cleaning_2.pl" class="section level4">
<h4><em>sentences-cleaning_2.pl</em></h4>
<ul>
<li>Remove spaces at the end of a sentence</li>
<li><p>More catching of profanities</p></li>
<li>TAGS
<ul>
<li>Consecutive identical TAGS</li>
<li>Remove TAGS enclosed in parenthesis</li>
<li>Remove USELESS TAGS</li>
</ul></li>
<li>Remove excess space
<ul>
<li>Removed excess space at the beginning</li>
<li>Removed excess space at the end</li>
<li>Removed excess space in the middle</li>
</ul></li>
<li>Clean <em>non-alpha</em> BEGINNING of sentences
<ul>
<li>Clean sentences ‚Äúbracketed‚Äù by quotes or parenthesis (removing the ‚Äúbracketing‚Äù character‚Äú)</li>
<li>Remove beginning quotes not paired in the rest of the sentence.</li>
<li>Bracketed TAGS - just remove them</li>
<li>Non-alpha preceding a TAG (done separately to make life simpler)</li>
<li>Catching up with more fixes for beginning</li>
<li>Clean sentences ‚Äúbracketed‚Äù by quotes or parenthesis (removing the ‚Äúbracketing‚Äù character‚Äú)</li>
</ul></li>
<li>Clean <em>non-alpha</em> ENDING of sentences
<ul>
<li>Clean extra spaces before ‚Äúgood‚Äù sentence endings</li>
<li>Cleaning orphan &quot; ‚Äô ) ] at the end match earlier in the sentence</li>
<li>Some dirty ENDS</li>
</ul></li>
</ul>
</div>
<div id="sentences-cleaning_3.pl" class="section level4">
<h4><em>sentences-cleaning_3.pl</em></h4>
<ul>
<li>CLEAN (again) lines BEGINNING with TAGS and non-alpha</li>
</ul>
<p><a name="APPENDIX-tokenization_issue"></a></p>
</div>
<div id="mysterious-issue-with-ngramtokenizer" class="section level3">
<h3>Mysterious issue with <code>NGramTokenizer</code></h3>
<p>Because the <code>NGramTokenizer</code> would fail with a <em>java memory error</em> if fed the full vector of sentences, but run when fed chunks of 100,000 sentences, I thought that turning this into a basic loop handling the splitting in chunks, collecting the output and finally return just one vector of <em>n-grams</em> would work, be compact and smarter.</p>
<p>It turns out that it fails‚Ä¶ and this puzzles me deeply.<br />Is R somehow handling the ‚Äústuff‚Äù in the loop in the same way it would if I run the tokenizer with the full vector?</p>
<p>Any clue?</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># NOT EVALUATED</span>
nl.chunk &lt;-<span class="st"> </span><span class="dv">100000</span>
N &lt;-<span class="st"> </span><span class="kw">ceiling</span>(<span class="kw">length</span>(sel.blogs.sentences)/nl.chunk)
alt.n3grams.blogs &lt;-<span class="st"> </span><span class="kw">vector</span>(<span class="st">&quot;list&quot;</span>, N)

<span class="kw">system.time</span>({
for( i in <span class="dv">1</span>:N ) {
    i &lt;-<span class="st"> </span>i<span class="dv">+1</span>
    n1 &lt;-<span class="st"> </span>(i<span class="dv">-1</span>)*nl.chunk +<span class="st"> </span><span class="dv">1</span>
    n2 &lt;-<span class="st"> </span><span class="kw">min</span>(i*nl.chunk, end.blogs)
    <span class="kw">cat</span>(<span class="st">&quot; &quot;</span>, i, n1, n2, <span class="st">&quot;</span><span class="ch">\n</span><span class="st">&quot;</span>)
    alt.n3grams.blogs[[i]] &lt;-<span class="st"> </span><span class="kw">NGramTokenizer</span>(sel.blogs.sentences[n1:n2], 
                                             <span class="kw">Weka_control</span>(<span class="dt">min =</span> <span class="dv">3</span>, <span class="dt">max =</span> <span class="dv">3</span>, 
                                                          <span class="dt">delimiters =</span> token_delim)) 
}
})</code></pre>
<hr />
</div>
</div>
</div>


</div>

<script>

// add bootstrap table styles to pandoc tables
$(document).ready(function () {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
});

</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
